{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "915e5e10",
   "metadata": {},
   "source": [
    "# My custom env for MARL\n",
    "- Importing my env \n",
    "- useing single agent\n",
    "- trying out SB3 \n",
    "\n",
    "Setting up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26ee4748",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box, Dict,Tuple, MultiDiscrete, MultiBinary\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from gymnasium import spaces\n",
    "import math\n",
    "\n",
    "from pettingzoo import AECEnv\n",
    "from gymnasium.utils import seeding\n",
    "\n",
    "from pettingzoo.utils import agent_selector\n",
    "\n",
    "from utils.physics_helpers import (\n",
    "    create_wall, create_cube_block, create_point_agent,\n",
    "    distance_between_bodies, create_fixed_constraint\n",
    ")\n",
    "\n",
    "\n",
    "class PyBulletPushEnv(Env):\n",
    "    # metadata = {\"render_modes\": [\"human\"], \"name\": \"pybullet_push_aec_v0\", \"is_parallelizable\": True}\n",
    "\n",
    "    def __init__(self, render_mode=None):\n",
    "        super().__init__()\n",
    "        self.numb_agents = 1\n",
    "        self.agents = [f\"agent_{i+1}\" for i in range(self.numb_agents)]\n",
    "        self.possible_agents = self.agents[:]\n",
    "\n",
    "        self.action_space =MultiDiscrete([5,2])\n",
    "        # â†’ [movement_action, grab_action]\n",
    "        # movement_action: 0=noop, 1=up, 2=down, 3=left, 4=right\n",
    "        # grab_action: 0 = do nothing, 1 = grab (or release if already grabbed)\n",
    "         \n",
    "        self.observation_space= Box(low=-3, high=30, shape=(self.numb_agents*2 + 6,), dtype=np.float32)\n",
    "        self.attached = {agent: False for agent in self.agents}\n",
    "\n",
    "        self.agent_name_mapping = {agent: i for i, agent in enumerate(self.agents)}\n",
    "\n",
    "        # self._agent_selector = agent_selector(self.agents)\n",
    "        # self.agent_selection = None\n",
    "        self.render_mode = render_mode\n",
    "        if self.render_mode == \"human\":\n",
    "            self.physics_client = p.connect(p.GUI)\n",
    "        else:\n",
    "            self.physics_client = p.connect(p.DIRECT)\n",
    "        # self.physics_client = p.connect(p.DIRECT)  # Use DIRECT mode for headless training (no GUI)\n",
    "        # self.physics_client = p.connect(p.GUI)  # Use DIRECT mode for headless training (no GUI)\n",
    "        \n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.time_step = 1. / 60.\n",
    "\n",
    "    def _setup_world(self):\n",
    "        if p.getConnectionInfo()['isConnected'] == 0:\n",
    "            if self.render_mode == \"human\":\n",
    "                p.connect(p.GUI)\n",
    "            else:\n",
    "                p.connect(p.DIRECT)\n",
    "            # p.connect(p.GUI)  # Use DIRECT mode for headless training (no GUI)\n",
    "        p.resetSimulation()\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "        create_wall(0, 3.0, 3.0, 0.05)\n",
    "        create_wall(0, -3.0, 3.0, 0.05)\n",
    "        create_wall(-3.0, 0.0, 0.05, 3.0)\n",
    "        create_wall(3.0, 0.0, 0.05, 3.0)\n",
    "        create_wall(0, 1.7, 0.05, 1.3)\n",
    "        create_wall(0, -1.7, 0.05, 1.3)\n",
    "\n",
    "        \n",
    "        rng = np.random.default_rng()                # optional: set a seed for repeatability\n",
    "        colour_palette = [\n",
    "            [1, 0, 0, 1],   # red\n",
    "            [0, 0, 1, 1],   # blue\n",
    "            [0, 1, 0, 1],   # green\n",
    "            [1, 1, 0, 1],   # yellow\n",
    "            [1, 0, 1, 1],   # magenta\n",
    "            [0, 1, 1, 1],   # cyan\n",
    "        ]\n",
    "        # self.agent_ids = {}\n",
    "        # for i in range(self.numb_agents):\n",
    "        #     name = f\"agent_{i+1}\"\n",
    "\n",
    "    #     # random spawn within the requested area\n",
    "        \n",
    "        cube_x_spawn = -0.5#rng.uniform(-2.5, -1.0)\n",
    "        cube_y_spawn =0#rng.uniform(-2.5,  2.5)\n",
    "        self.cube_id = create_cube_block(cube_x_spawn, cube_y_spawn)\n",
    "        x_spawn = rng.uniform(-2.5, -1.0)\n",
    "        y_spawn = rng.uniform(-2.5,  2.5)\n",
    "\n",
    "        #     colour  = colour_palette[i % len(colour_palette)]\n",
    "        #     self.agent_ids[name] = create_point_agent(x_spawn, y_spawn, color=colour)\n",
    "        self.agent_ids = {\n",
    "            \"agent_1\": create_point_agent(x_spawn, y_spawn, color=[1, 0, 0, 1]),\n",
    "            # \"agent_1\": create_point_agent(-0.5, -2.5, color=[0, 0, 1, 1])\n",
    "        }\n",
    "        self.attach_constraint_ids = {agent: None for agent in self.agents}\n",
    "\n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "    def reset(self,*, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        self._setup_world()\n",
    "        # if seed is not None:\n",
    "        #     self.np_random, self.np_random_seed = seeding.np_random(seed)\n",
    "\n",
    "        self.agents = self.possible_agents[:]\n",
    "        self.rewards = {agent: 0 for agent in self.agents}\n",
    "        self._cumulative_rewards = {agent: 0 for agent in self.agents}\n",
    "        self.terminations = {agent: False for agent in self.agents}\n",
    "        self.truncations = {agent: False for agent in self.agents}\n",
    "        self.infos = {agent: {} for agent in self.agents}\n",
    "        self._agent_steps = {agent: 0 for agent in self.agents}\n",
    "        self.previous_dist= 0\n",
    "        self.cube_previous_dist= 0\n",
    "\n",
    "        self.attached = {agent: False for agent in self.agents}\n",
    "        self.goal_pose = [2.0,0.0]\n",
    "\n",
    "        # self._agent_selector = agent_selector(self.agents)\n",
    "        # self.agent_selection = self._agent_selector.reset()\n",
    "        #Get observation\n",
    "        agent_id = self.agent_ids[self.agents[0]]\n",
    "        agent_pos, _ = p.getBasePositionAndOrientation(agent_id)\n",
    "        cube_pos, _ = p.getBasePositionAndOrientation(self.cube_id)\n",
    "        dist_btw_cube_agent = distance_between_bodies(agent_id, self.cube_id)\n",
    "        goal_distance = np.linalg.norm(np.array(cube_pos[:2]) - np.array(self.goal_pose[:2]))\n",
    "\n",
    "\n",
    "        self.obs = np.array(agent_pos[:2] + cube_pos[:2]+tuple(self.goal_pose)+(dist_btw_cube_agent,)+(goal_distance,), dtype=np.float32)\n",
    "        self.info = {}\n",
    "        return self.obs, self.info\n",
    "\n",
    "    def step(self, action):\n",
    "        # if self.terminations[self.agent_selection] or self.truncations[self.agent_selection]:\n",
    "        #     self._was_dead_step(action)\n",
    "        #     return\n",
    "        movement_action = action[0]\n",
    "        grab_action = action[1]\n",
    "        \n",
    "\n",
    "        # agent = self.agent_selection\n",
    "        for agent in self.agents:\n",
    "            if self.terminations[agent] or self.truncations[agent]:\n",
    "                continue\n",
    "            agent_id = self.agent_ids[agent]\n",
    "            # Clears the instant rewards for the current agent from prev iteration before applying the action\n",
    "            # self._clear_rewards()\n",
    "            success_bonus = 100.0  # Reward bonus for reaching the goal\n",
    "            vx, vy = 0, 0\n",
    "            speed = 5.5\n",
    "            if movement_action == 1: vy = speed\n",
    "            elif movement_action == 2: vy = -speed\n",
    "            elif movement_action == 3: vx = -speed\n",
    "            elif movement_action == 4: vx = speed\n",
    "\n",
    "            if grab_action == 1:\n",
    "                if not self.attached[agent]:\n",
    "                    self.attach_constraint_ids[agent] = create_fixed_constraint(\n",
    "                            agent_id, self.cube_id, grab_distance_threshold=0.2\n",
    "                        )\n",
    "                    if self.attach_constraint_ids[agent] is not None:\n",
    "                        self.attached[agent] = True\n",
    "                        attach_reward = 12 # Reward for successfully attaching to the cube\n",
    "                        self.start_time = self._agent_steps[agent] # Store the start time of the attachment\n",
    "                    else:\n",
    "                        attach_reward = 0 # No reward if the attachment fails\n",
    "                        self.start_time = 0\n",
    "\n",
    "                else:\n",
    "                    attach_reward = 0.100 * math.exp(-0.01*(self._agent_steps[agent]-self.start_time))# Reward for holding the cube\n",
    "                    \n",
    "\n",
    "            elif grab_action == 0:\n",
    "                if self.attached[agent]:\n",
    "                    p.removeConstraint(self.attach_constraint_ids[agent])\n",
    "                    self.attach_constraint_ids[agent] = None\n",
    "                    self.attached[agent] = False\n",
    "                    attach_reward = -18 # Reward for releasing the cube\n",
    "\n",
    "                    \n",
    "\n",
    "                else:\n",
    "                    attach_reward = -0.5 # Reward for not holding the cube\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            p.resetBaseVelocity(agent_id, [vx, vy, 0])\n",
    "            p.stepSimulation()\n",
    "            if self.render_mode == \"human\":\n",
    "                time.sleep(self.time_step)\n",
    "\n",
    "            # Reward based on distance to cube\n",
    "            dist_between_cube_agent = distance_between_bodies(agent_id, self.cube_id)\n",
    "            # if self.previous_dist == 0:\n",
    "            #     incentive = 0\n",
    "            # else:\n",
    "            #     incentive= (self.previous_dist - dist_between_cube_agent)\n",
    "            # self.previous_dist = dist_between_cube_agent\n",
    "            # expo_dist1=0.2*math.exp(-0.4*dist_between_cube_agent)\n",
    "            \n",
    "\n",
    "            # Reward based on distance to goal\n",
    "            agent_pos, _ = p.getBasePositionAndOrientation(agent_id)\n",
    "            cube_pos, _ = p.getBasePositionAndOrientation(self.cube_id)\n",
    "            goal_pos = self.goal_pose\n",
    "            goal_distance = np.linalg.norm(np.array(cube_pos[:2]) - np.array(goal_pos[:2]))\n",
    "            # if self.attached[agent]:\n",
    "            #     if self.cube_previous_dist == 0:\n",
    "            #         cube_incentive = 0\n",
    "            #     else:\n",
    "            #         cube_incentive = (self.cube_previous_dist - goal_distance)\n",
    "            # else:\n",
    "            #     cube_incentive = 0\n",
    "            # self.cube_previous_dist = goal_distance\n",
    "\n",
    "            if self.attached[agent]:\n",
    "                if not hasattr(self, \"cube_previous_dist\") or self.cube_previous_dist is None:\n",
    "                    cube_incentive = 0\n",
    "                else:\n",
    "                    cube_incentive = 300*(self.cube_previous_dist - goal_distance)\n",
    "                self.cube_previous_dist = goal_distance\n",
    "                expo_dist=1.0*math.exp(-0.4*goal_distance)\n",
    "                expo_dist1=0.2*math.exp(-0.03*(self._agent_steps[agent]-self.start_time))\n",
    "                incentive = 0\n",
    "\n",
    "            else:\n",
    "                if self.previous_dist == 0:\n",
    "                    incentive = 0\n",
    "                else:\n",
    "                    incentive= (self.previous_dist - dist_between_cube_agent)\n",
    "                expo_dist1=0.4*math.exp(-0.4*dist_between_cube_agent)\n",
    "                cube_incentive = 0\n",
    "                self.cube_previous_dist = None\n",
    "                expo_dist=0\n",
    "            self.previous_dist = dist_between_cube_agent\n",
    "            \n",
    "\n",
    "            \n",
    "            self.rewards[agent] = 20*incentive+expo_dist1+attach_reward+cube_incentive+expo_dist-0.0001 * self._agent_steps[agent] \n",
    "            self._cumulative_rewards[agent]+=self.rewards[agent]\n",
    "            # self._cumulative_rewards[agent] += self.rewards[agent]\n",
    "            # print(\"AT \", self._agent_steps[agent] , agent,\"took action: \", action ,\"REW:\", self.rewards[agent], \"CUM_REW:\", self._cumulative_rewards[agent])\n",
    "\n",
    "            # # Termination Check\n",
    "            # cube_pos,_ =  p.getBasePositionAndOrientation(self.cube_id)\n",
    "            goal_threshold = 0.2\n",
    "            agent_pos, _ = p.getBasePositionAndOrientation(agent_id)\n",
    "            self.obs = np.array(agent_pos[:2] + cube_pos[:2]+tuple(self.goal_pose)+(dist_between_cube_agent,)+(goal_distance,), dtype=np.float32)\n",
    "\n",
    "            # self.obs = np.array(agent_pos[:2] + cube_pos[:2]+ (dist_between_cube_agent,), dtype=np.float32)\n",
    "            cube_to_agent_distance = np.linalg.norm(np.array(agent_pos[:2]) - np.array(cube_pos[:2]))\n",
    "\n",
    "            if  goal_distance<goal_threshold:#self.attached[agent]: #cube_to_agent_distance < goal_threshold:\n",
    "                self.rewards[agent] += success_bonus\n",
    "                self.terminations[agent] = True\n",
    "                self.truncations[agent] = False\n",
    "                print(\"Terminnation due to goal reached at step: \", self._agent_steps[agent])\n",
    "                break\n",
    "\n",
    "            else:\n",
    "                self.terminations[agent] = False\n",
    "\n",
    "            # self._accumulate_rewards()\n",
    "            \n",
    "            # Truncation Check\n",
    "            # max_steps = 500\n",
    "            # if self._agent_steps[agent] >= max_steps:\n",
    "            #     self.truncations[agent] = True\n",
    "            #     self.terminations[agent] = False\n",
    "            # else:\n",
    "            #     self.truncations[agent] = False\n",
    "             # Truncation check\n",
    "            self._agent_steps[agent] += 1\n",
    "            if self._agent_steps[agent] >= 500:\n",
    "                self.terminations[agent] =True\n",
    "                break\n",
    "            \n",
    "\n",
    "            # Info dict\n",
    "            self.info = {}\n",
    "            # print(\"AT TP \", self._agent_steps[agent],\" CR: \", self._cumulative_rewards[agent], \" RW: \", self.rewards[agent], \" IN: \", incentive,\"exp_dist1: \",expo_dist1, \" AR: \", attach_reward, \"EXp: \",expo_dist, \"CIN: \", cube_incentive)\n",
    "\n",
    "            \n",
    "            \n",
    "            # self.agent_selection = self._agent_selector.next()\n",
    "            \n",
    "            if self.render_mode == \"human\":\n",
    "                self.render()\n",
    "        return self.obs, self.rewards[agent], self.terminations[agent], self.truncations[agent], self.info\n",
    "\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def action_space(self, agent):\n",
    "        # We can seed the action space to make the environment deterministic.\n",
    "        return Discrete(5, seed=self.np_random_seed)\n",
    "    def observe(self, agent):\n",
    "        agent_ids = self.agent_ids.copy()\n",
    "        current_agent_pos, _ = p.getBasePositionAndOrientation(agent_ids[agent])\n",
    "        cube_pos, _ = p.getBasePositionAndOrientation(self.cube_id)\n",
    "\n",
    "        other_agent_poses = []\n",
    "        for other_agent in self.agents:\n",
    "            if other_agent != agent:\n",
    "                pos, _ = p.getBasePositionAndOrientation(agent_ids[other_agent])\n",
    "                other_agent_poses.extend(pos[:2])\n",
    "\n",
    "        obs = np.array(\n",
    "            list(current_agent_pos[:2]) + list(cube_pos[:2]) + other_agent_poses,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        return obs\n",
    "\n",
    "    def render(self):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b50f14",
   "metadata": {},
   "source": [
    "#### Just to Debug the attributees of the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "595836ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(-3.0, 30.0, (8,), float32)\n",
      "MultiDiscrete([5 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([-1.3010845 , -1.9500062 ,  1.8254256 , -0.79885143,  2.        ,\n",
       "         0.        ,  3.3320749 ,  0.817704  ], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = PyBulletPushEnv() # Creating the environment\n",
    "print(env.observation_space)\n",
    "print(env.action_space) # Printing the action space of the environment\n",
    "env.reset() # Resetting the environment\n",
    "# print(env.state)\n",
    "# print(env.step(1)) # Taking a step in the environment with action 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f72e8",
   "metadata": {},
   "source": [
    "## Test Environment\n",
    "- This checks whether there is any error with the environrment, checks step(), reset() methds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797f771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminnation due to goal reached at step:  0\n",
      "obs: [-2.2948344   0.00698965  1.8543851   0.02557174  2.          0.\n",
      "  4.1495624   0.14784321], Done: True, Reward: 99.57606890852344, Truncated: False\n",
      "Episode 1 finished with score: 99.57606890852344\n",
      "obs: [-0.6613488  -1.0255641   2.3979764   0.01275994  2.          0.\n",
      "  3.2311125   0.398181  ], Done: True, Reward: -0.3717784734771555, Truncated: False\n",
      "Episode 2 finished with score: -72.52951251860672\n",
      "obs: [-2.8807201  -0.28292492  1.196542    0.924081    2.          0.\n",
      "  4.252462    1.2245287 ], Done: True, Reward: 0.15364447115667082, Truncated: False\n",
      "Episode 3 finished with score: -110.15412257139089\n",
      "obs: [-2.106749   -0.2351393   2.32866    -0.94676757  2.          0.\n",
      "  4.492412    1.0021907 ], Done: True, Reward: -0.4338979758470881, Truncated: False\n",
      "Episode 4 finished with score: -94.06800961669609\n",
      "obs: [-1.6734421  -1.6587591   2.3412888  -0.36922526  2.          0.\n",
      "  4.2170444   0.50279754], Done: True, Reward: -0.9099886167465459, Truncated: False\n",
      "Episode 5 finished with score: -96.79972978127796\n",
      "obs: [-1.4912152 -1.4731843  1.2921764 -2.3356867  2.         0.\n",
      "  2.9143918  2.4405832], Done: True, Reward: -0.05862024479437178, Truncated: False\n",
      "Episode 6 finished with score: -72.55128921429987\n",
      "obs: [-1.6159661  -0.09984609  2.4987893   0.07921369  2.          0.\n",
      "  4.118953    0.50504017], Done: True, Reward: 0.027379474260126546, Truncated: False\n",
      "Episode 7 finished with score: -105.49882182137199\n",
      "obs: [-2.0978997 -1.9083458  2.1949453 -2.1526248  2.         0.\n",
      "  4.3000803  2.1614342], Done: True, Reward: -0.9336371597623476, Truncated: False\n",
      "Episode 8 finished with score: -95.25880698685368\n",
      "obs: [-2.2344134 -1.6278665  1.0987031  1.3303721  2.         0.\n",
      "  4.456831   1.6069306], Done: True, Reward: -0.32323432537404934, Truncated: False\n",
      "Episode 9 finished with score: -106.48706483843263\n",
      "obs: [-2.1347764  -0.5651523   1.4939051  -0.31081524  2.          0.\n",
      "  3.6379275   0.59391767], Done: True, Reward: -0.4560714596165519, Truncated: False\n",
      "Episode 10 finished with score: -93.49589477138805\n"
     ]
    }
   ],
   "source": [
    "env = PyBulletPushEnv() # Creating the environment\n",
    "episode=10\n",
    "for episode in range(1, episode + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs, info = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action= env.action_space.sample()  # Random action for demonstration\n",
    "        # print(f\"Action: {action}\")\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        if done:\n",
    "            # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "            print(f\"obs: {obs}, Done: {done}, Reward: {reward}, Truncated: {truncated}\")\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec58a88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Need to disconnect the pybullet client otherwise it will keep running in the background\n",
    "# env.close() # Close the environment\n",
    "p.disconnect() # Disconnect the pybullet client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc05a52",
   "metadata": {},
   "source": [
    "### Training My Agent\n",
    " - Createing variable for saveing logs locations\n",
    " - First part trains on one environment for the given time steps\n",
    " - Second part vectorizez the environemnt and trains parallel environement at the same time to decrease the training time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3170b531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\Logs\\PPO_20\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 500      |\n",
      "|    ep_rew_mean     | -50.4    |\n",
      "| time/              |          |\n",
      "|    fps             | 2127     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | -30.2       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1420        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023294041 |\n",
      "|    clip_fraction        | 0.244       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.29       |\n",
      "|    explained_variance   | -0.0926     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.536       |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0279     |\n",
      "|    value_loss           | 1.51        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | -10.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1238        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013510605 |\n",
      "|    clip_fraction        | 0.255       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.26       |\n",
      "|    explained_variance   | 0.169       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.539       |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0321     |\n",
      "|    value_loss           | 1.15        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 3.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1171        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014937082 |\n",
      "|    clip_fraction        | 0.254       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.22       |\n",
      "|    explained_variance   | 0.161       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.472       |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.031      |\n",
      "|    value_loss           | 2.14        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 15.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1149       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 8          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01779783 |\n",
      "|    clip_fraction        | 0.281      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.18      |\n",
      "|    explained_variance   | 0.45       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.753      |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0349    |\n",
      "|    value_loss           | 1.67       |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 20.2       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1133       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01820555 |\n",
      "|    clip_fraction        | 0.311      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -2.1       |\n",
      "|    explained_variance   | 0.368      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 0.666      |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0355    |\n",
      "|    value_loss           | 1.81       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 29.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1125        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015626317 |\n",
      "|    clip_fraction        | 0.098       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -2.05       |\n",
      "|    explained_variance   | 0.0673      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 38          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1121        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 14          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014710495 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.396       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.48        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0149     |\n",
      "|    value_loss           | 8.08        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 39.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1103        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 16          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011098625 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.93       |\n",
      "|    explained_variance   | 0.495       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.68        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 7.64        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 47.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1098        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012845532 |\n",
      "|    clip_fraction        | 0.0989      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.88       |\n",
      "|    explained_variance   | 0.464       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.2        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 28.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 57.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1099        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015691468 |\n",
      "|    clip_fraction        | 0.169       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.8        |\n",
      "|    explained_variance   | 0.815       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.459       |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.013      |\n",
      "|    value_loss           | 3.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 64.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1101        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010435458 |\n",
      "|    clip_fraction        | 0.0954      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.71       |\n",
      "|    explained_variance   | 0.722       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.58        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 71.2         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1098         |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 26624        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0051104976 |\n",
      "|    clip_fraction        | 0.0411       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0.801        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.29         |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.0058      |\n",
      "|    value_loss           | 9.24         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 76.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1096        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 26          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009228215 |\n",
      "|    clip_fraction        | 0.038       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.67       |\n",
      "|    explained_variance   | 0.818       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.85        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00466    |\n",
      "|    value_loss           | 13.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 82.1         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1095         |\n",
      "|    iterations           | 15           |\n",
      "|    time_elapsed         | 28           |\n",
      "|    total_timesteps      | 30720        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062055197 |\n",
      "|    clip_fraction        | 0.0418       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.66        |\n",
      "|    explained_variance   | 0.842        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.67         |\n",
      "|    n_updates            | 140          |\n",
      "|    policy_gradient_loss | -0.00527     |\n",
      "|    value_loss           | 14.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 87.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1090        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005473605 |\n",
      "|    clip_fraction        | 0.0209      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.63       |\n",
      "|    explained_variance   | 0.845       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.19        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00446    |\n",
      "|    value_loss           | 12.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 90.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1085        |\n",
      "|    iterations           | 17          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 34816       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010144287 |\n",
      "|    clip_fraction        | 0.102       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.54       |\n",
      "|    explained_variance   | 0.755       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.1         |\n",
      "|    n_updates            | 160         |\n",
      "|    policy_gradient_loss | -0.00693    |\n",
      "|    value_loss           | 12.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 93.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1081        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014363139 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.5        |\n",
      "|    explained_variance   | 0.806       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.58        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00651    |\n",
      "|    value_loss           | 9.83        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 96.3       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1078       |\n",
      "|    iterations           | 19         |\n",
      "|    time_elapsed         | 36         |\n",
      "|    total_timesteps      | 38912      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01152973 |\n",
      "|    clip_fraction        | 0.103      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.5       |\n",
      "|    explained_variance   | 0.767      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 5.16       |\n",
      "|    n_updates            | 180        |\n",
      "|    policy_gradient_loss | -0.00744   |\n",
      "|    value_loss           | 16.5       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 100         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1074        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 38          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012232203 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.577       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.53        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00797    |\n",
      "|    value_loss           | 10.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 92          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1070        |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013716398 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.7         |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.23        |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.00971    |\n",
      "|    value_loss           | 12.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 95.3         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1067         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 42           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0122984145 |\n",
      "|    clip_fraction        | 0.111        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.41        |\n",
      "|    explained_variance   | 0.108        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.67         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.00461     |\n",
      "|    value_loss           | 577          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 97.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1065        |\n",
      "|    iterations           | 23          |\n",
      "|    time_elapsed         | 44          |\n",
      "|    total_timesteps      | 47104       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013469845 |\n",
      "|    clip_fraction        | 0.178       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.883       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.29        |\n",
      "|    n_updates            | 220         |\n",
      "|    policy_gradient_loss | -0.00773    |\n",
      "|    value_loss           | 6.17        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 87.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1064        |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 46          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011810636 |\n",
      "|    clip_fraction        | 0.231       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.49       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.19        |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | -0.0168     |\n",
      "|    value_loss           | 6.6         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 93          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1060        |\n",
      "|    iterations           | 25          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 51200       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011608381 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.47       |\n",
      "|    explained_variance   | 0.0706      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.82        |\n",
      "|    n_updates            | 240         |\n",
      "|    policy_gradient_loss | -0.00875    |\n",
      "|    value_loss           | 738         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 86.7         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1058         |\n",
      "|    iterations           | 26           |\n",
      "|    time_elapsed         | 50           |\n",
      "|    total_timesteps      | 53248        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0092729665 |\n",
      "|    clip_fraction        | 0.0956       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.43        |\n",
      "|    explained_variance   | 0.0479       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.78         |\n",
      "|    n_updates            | 250          |\n",
      "|    policy_gradient_loss | -0.00384     |\n",
      "|    value_loss           | 872          |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 93.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010462849 |\n",
      "|    clip_fraction        | 0.0729      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.849       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.1         |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.00401    |\n",
      "|    value_loss           | 13.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 98.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1053        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 54          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011592475 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.75        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.00684    |\n",
      "|    value_loss           | 9.81        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 104         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1053        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013713649 |\n",
      "|    clip_fraction        | 0.158       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.823       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.13        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | -0.0019     |\n",
      "|    value_loss           | 7.34        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 110          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1053         |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 58           |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0059742667 |\n",
      "|    clip_fraction        | 0.0872       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.38        |\n",
      "|    explained_variance   | 0.887        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.11         |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -0.00513     |\n",
      "|    value_loss           | 8.27         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 113         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011459356 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.38       |\n",
      "|    explained_variance   | 0.633       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.27        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00764    |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 117         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008236412 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.667       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.7        |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | -0.00263    |\n",
      "|    value_loss           | 18.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 122         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1054        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 64          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007885093 |\n",
      "|    clip_fraction        | 0.105       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.694       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.55        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | -0.00543    |\n",
      "|    value_loss           | 22.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 126          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1054         |\n",
      "|    iterations           | 34           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 69632        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0069860388 |\n",
      "|    clip_fraction        | 0.0701       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.754        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.73         |\n",
      "|    n_updates            | 330          |\n",
      "|    policy_gradient_loss | -0.00146     |\n",
      "|    value_loss           | 13.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 128         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1055        |\n",
      "|    iterations           | 35          |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 71680       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011045558 |\n",
      "|    clip_fraction        | 0.131       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.871       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.34        |\n",
      "|    n_updates            | 340         |\n",
      "|    policy_gradient_loss | -0.00293    |\n",
      "|    value_loss           | 10.2        |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 500       |\n",
      "|    ep_rew_mean          | 129       |\n",
      "| time/                   |           |\n",
      "|    fps                  | 1057      |\n",
      "|    iterations           | 36        |\n",
      "|    time_elapsed         | 69        |\n",
      "|    total_timesteps      | 73728     |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0119623 |\n",
      "|    clip_fraction        | 0.0855    |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -1.18     |\n",
      "|    explained_variance   | 0.796     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.77      |\n",
      "|    n_updates            | 350       |\n",
      "|    policy_gradient_loss | -0.00378  |\n",
      "|    value_loss           | 19.5      |\n",
      "---------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 130         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1059        |\n",
      "|    iterations           | 37          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 75776       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016673587 |\n",
      "|    clip_fraction        | 0.177       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.25       |\n",
      "|    explained_variance   | 0.821       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.54        |\n",
      "|    n_updates            | 360         |\n",
      "|    policy_gradient_loss | -0.00199    |\n",
      "|    value_loss           | 12          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 132         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1061        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 73          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005236252 |\n",
      "|    clip_fraction        | 0.0997      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.963       |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | -0.00211    |\n",
      "|    value_loss           | 10          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 132         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1063        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 75          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006813077 |\n",
      "|    clip_fraction        | 0.113       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.837       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.01        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | -0.00213    |\n",
      "|    value_loss           | 10.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 133         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1063        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 76          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003923458 |\n",
      "|    clip_fraction        | 0.0827      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.905       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.06        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.00325    |\n",
      "|    value_loss           | 9.21        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 134         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1065        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 78          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012229465 |\n",
      "|    clip_fraction        | 0.129       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.98        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | -0.0033     |\n",
      "|    value_loss           | 7.47        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 136         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1066        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 80          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011941411 |\n",
      "|    clip_fraction        | 0.116       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.11       |\n",
      "|    explained_variance   | 0.869       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.942       |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | -0.00392    |\n",
      "|    value_loss           | 9.18        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1066        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 82          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013067175 |\n",
      "|    clip_fraction        | 0.15        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.16       |\n",
      "|    explained_variance   | 0.736       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.24        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | -0.00668    |\n",
      "|    value_loss           | 13.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 138         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1068        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 84          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.026514161 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.816       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.53        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | -0.0109     |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 137         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1069        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 86          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015053907 |\n",
      "|    clip_fraction        | 0.179       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.757       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.04        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | -0.00331    |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 148        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1070       |\n",
      "|    iterations           | 46         |\n",
      "|    time_elapsed         | 87         |\n",
      "|    total_timesteps      | 94208      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01079729 |\n",
      "|    clip_fraction        | 0.155      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.24      |\n",
      "|    explained_variance   | 0.695      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 1.92       |\n",
      "|    n_updates            | 450        |\n",
      "|    policy_gradient_loss | -0.00549   |\n",
      "|    value_loss           | 12.4       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 148         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1071        |\n",
      "|    iterations           | 47          |\n",
      "|    time_elapsed         | 89          |\n",
      "|    total_timesteps      | 96256       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010456877 |\n",
      "|    clip_fraction        | 0.124       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.521       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.46        |\n",
      "|    n_updates            | 460         |\n",
      "|    policy_gradient_loss | -0.00261    |\n",
      "|    value_loss           | 11          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 149         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1071        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 91          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010090414 |\n",
      "|    clip_fraction        | 0.122       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.19       |\n",
      "|    explained_variance   | 0.721       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.01        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | -0.000467   |\n",
      "|    value_loss           | 10.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 162          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1069         |\n",
      "|    iterations           | 49           |\n",
      "|    time_elapsed         | 93           |\n",
      "|    total_timesteps      | 100352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0068595065 |\n",
      "|    clip_fraction        | 0.145        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.78         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.72         |\n",
      "|    n_updates            | 480          |\n",
      "|    policy_gradient_loss | -0.00711     |\n",
      "|    value_loss           | 9.29         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 178          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1068         |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 95           |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0062214835 |\n",
      "|    clip_fraction        | 0.0648       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.22        |\n",
      "|    explained_variance   | 0.813        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.56         |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 9.24         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 179         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1068        |\n",
      "|    iterations           | 51          |\n",
      "|    time_elapsed         | 97          |\n",
      "|    total_timesteps      | 104448      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010010957 |\n",
      "|    clip_fraction        | 0.132       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.24       |\n",
      "|    explained_variance   | 0.67        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.94        |\n",
      "|    n_updates            | 500         |\n",
      "|    policy_gradient_loss | -0.000838   |\n",
      "|    value_loss           | 11.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 178        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1068       |\n",
      "|    iterations           | 52         |\n",
      "|    time_elapsed         | 99         |\n",
      "|    total_timesteps      | 106496     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.03967751 |\n",
      "|    clip_fraction        | 0.308      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.37      |\n",
      "|    explained_variance   | 0.917      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.82       |\n",
      "|    n_updates            | 510        |\n",
      "|    policy_gradient_loss | -0.0233    |\n",
      "|    value_loss           | 7.45       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 180         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1068        |\n",
      "|    iterations           | 53          |\n",
      "|    time_elapsed         | 101         |\n",
      "|    total_timesteps      | 108544      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015157517 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.745       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.38        |\n",
      "|    n_updates            | 520         |\n",
      "|    policy_gradient_loss | -0.00452    |\n",
      "|    value_loss           | 12.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 180        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1067       |\n",
      "|    iterations           | 54         |\n",
      "|    time_elapsed         | 103        |\n",
      "|    total_timesteps      | 110592     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00958363 |\n",
      "|    clip_fraction        | 0.111      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.38      |\n",
      "|    explained_variance   | 0.486      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 6.08       |\n",
      "|    n_updates            | 530        |\n",
      "|    policy_gradient_loss | -0.00804   |\n",
      "|    value_loss           | 15.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 180         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1066        |\n",
      "|    iterations           | 55          |\n",
      "|    time_elapsed         | 105         |\n",
      "|    total_timesteps      | 112640      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007919718 |\n",
      "|    clip_fraction        | 0.109       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.39       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.58        |\n",
      "|    n_updates            | 540         |\n",
      "|    policy_gradient_loss | -0.00435    |\n",
      "|    value_loss           | 11.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 180         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1067        |\n",
      "|    iterations           | 56          |\n",
      "|    time_elapsed         | 107         |\n",
      "|    total_timesteps      | 114688      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011970863 |\n",
      "|    clip_fraction        | 0.197       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.897       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.15        |\n",
      "|    n_updates            | 550         |\n",
      "|    policy_gradient_loss | -0.00677    |\n",
      "|    value_loss           | 11.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 180        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1067       |\n",
      "|    iterations           | 57         |\n",
      "|    time_elapsed         | 109        |\n",
      "|    total_timesteps      | 116736     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01183568 |\n",
      "|    clip_fraction        | 0.142      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.42      |\n",
      "|    explained_variance   | 0.854      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.34       |\n",
      "|    n_updates            | 560        |\n",
      "|    policy_gradient_loss | -0.00278   |\n",
      "|    value_loss           | 13         |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 178         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1067        |\n",
      "|    iterations           | 58          |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 118784      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012252141 |\n",
      "|    clip_fraction        | 0.186       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.31       |\n",
      "|    explained_variance   | 0.659       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.13        |\n",
      "|    n_updates            | 570         |\n",
      "|    policy_gradient_loss | -0.0037     |\n",
      "|    value_loss           | 14.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 177         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1066        |\n",
      "|    iterations           | 59          |\n",
      "|    time_elapsed         | 113         |\n",
      "|    total_timesteps      | 120832      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017190823 |\n",
      "|    clip_fraction        | 0.204       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.692       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.71        |\n",
      "|    n_updates            | 580         |\n",
      "|    policy_gradient_loss | -0.00524    |\n",
      "|    value_loss           | 15.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 177         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1067        |\n",
      "|    iterations           | 60          |\n",
      "|    time_elapsed         | 115         |\n",
      "|    total_timesteps      | 122880      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006658894 |\n",
      "|    clip_fraction        | 0.0946      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.825       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.39        |\n",
      "|    n_updates            | 590         |\n",
      "|    policy_gradient_loss | -0.00292    |\n",
      "|    value_loss           | 8.68        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 177         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1068        |\n",
      "|    iterations           | 61          |\n",
      "|    time_elapsed         | 116         |\n",
      "|    total_timesteps      | 124928      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.020395562 |\n",
      "|    clip_fraction        | 0.111       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.877       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.41        |\n",
      "|    n_updates            | 600         |\n",
      "|    policy_gradient_loss | -0.00344    |\n",
      "|    value_loss           | 7.91        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 176          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1068         |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 118          |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097764805 |\n",
      "|    clip_fraction        | 0.119        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.32        |\n",
      "|    explained_variance   | 0.911        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.16         |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -0.00313     |\n",
      "|    value_loss           | 8.67         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 177         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1069        |\n",
      "|    iterations           | 63          |\n",
      "|    time_elapsed         | 120         |\n",
      "|    total_timesteps      | 129024      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014408034 |\n",
      "|    clip_fraction        | 0.212       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.43       |\n",
      "|    explained_variance   | 0.481       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.9        |\n",
      "|    n_updates            | 620         |\n",
      "|    policy_gradient_loss | -0.0051     |\n",
      "|    value_loss           | 14.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 178          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1069         |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 122          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0048087477 |\n",
      "|    clip_fraction        | 0.118        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.755        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.67         |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.00459     |\n",
      "|    value_loss           | 12.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 179          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1070         |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 124          |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0076047555 |\n",
      "|    clip_fraction        | 0.0472       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.851        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.13         |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -0.00248     |\n",
      "|    value_loss           | 11           |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 180         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1071        |\n",
      "|    iterations           | 66          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 135168      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011258131 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.29       |\n",
      "|    explained_variance   | 0.793       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.93        |\n",
      "|    n_updates            | 650         |\n",
      "|    policy_gradient_loss | -0.00438    |\n",
      "|    value_loss           | 14.8        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 180        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1071       |\n",
      "|    iterations           | 67         |\n",
      "|    time_elapsed         | 128        |\n",
      "|    total_timesteps      | 137216     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01403973 |\n",
      "|    clip_fraction        | 0.124      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.39      |\n",
      "|    explained_variance   | 0.888      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 4.75       |\n",
      "|    n_updates            | 660        |\n",
      "|    policy_gradient_loss | -0.00444   |\n",
      "|    value_loss           | 8.16       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 181          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1072         |\n",
      "|    iterations           | 68           |\n",
      "|    time_elapsed         | 129          |\n",
      "|    total_timesteps      | 139264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0107457405 |\n",
      "|    clip_fraction        | 0.137        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.4         |\n",
      "|    explained_variance   | 0.809        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42         |\n",
      "|    n_updates            | 670          |\n",
      "|    policy_gradient_loss | -0.00334     |\n",
      "|    value_loss           | 10.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 182         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1072        |\n",
      "|    iterations           | 69          |\n",
      "|    time_elapsed         | 131         |\n",
      "|    total_timesteps      | 141312      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008672392 |\n",
      "|    clip_fraction        | 0.0984      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.834       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.51        |\n",
      "|    n_updates            | 680         |\n",
      "|    policy_gradient_loss | -0.00531    |\n",
      "|    value_loss           | 16          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 182         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1073        |\n",
      "|    iterations           | 70          |\n",
      "|    time_elapsed         | 133         |\n",
      "|    total_timesteps      | 143360      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008124257 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.41       |\n",
      "|    explained_variance   | 0.799       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.9        |\n",
      "|    n_updates            | 690         |\n",
      "|    policy_gradient_loss | 0.00112     |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 184         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1074        |\n",
      "|    iterations           | 71          |\n",
      "|    time_elapsed         | 135         |\n",
      "|    total_timesteps      | 145408      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008906578 |\n",
      "|    clip_fraction        | 0.175       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.34       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.68        |\n",
      "|    n_updates            | 700         |\n",
      "|    policy_gradient_loss | 0.00184     |\n",
      "|    value_loss           | 9.71        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 185         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1074        |\n",
      "|    iterations           | 72          |\n",
      "|    time_elapsed         | 137         |\n",
      "|    total_timesteps      | 147456      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004097583 |\n",
      "|    clip_fraction        | 0.115       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.27       |\n",
      "|    explained_variance   | 0.668       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.05        |\n",
      "|    n_updates            | 710         |\n",
      "|    policy_gradient_loss | -3.52e-05   |\n",
      "|    value_loss           | 18.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 185         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1075        |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 139         |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011619598 |\n",
      "|    clip_fraction        | 0.133       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.782       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.82        |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -0.0052     |\n",
      "|    value_loss           | 13.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 183          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1076         |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 140          |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075823637 |\n",
      "|    clip_fraction        | 0.109        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.27        |\n",
      "|    explained_variance   | 0.815        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.59         |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -0.000737    |\n",
      "|    value_loss           | 14.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 183         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1077        |\n",
      "|    iterations           | 75          |\n",
      "|    time_elapsed         | 142         |\n",
      "|    total_timesteps      | 153600      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016394958 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.18       |\n",
      "|    explained_variance   | 0.831       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3.38        |\n",
      "|    n_updates            | 740         |\n",
      "|    policy_gradient_loss | -0.000878   |\n",
      "|    value_loss           | 11.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 184          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1078         |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 144          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073152585 |\n",
      "|    clip_fraction        | 0.0761       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.23        |\n",
      "|    explained_variance   | 0.79         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 12.3         |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.00235     |\n",
      "|    value_loss           | 14.7         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 184         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1078        |\n",
      "|    iterations           | 77          |\n",
      "|    time_elapsed         | 146         |\n",
      "|    total_timesteps      | 157696      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009479016 |\n",
      "|    clip_fraction        | 0.123       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.579       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77        |\n",
      "|    n_updates            | 760         |\n",
      "|    policy_gradient_loss | -0.00186    |\n",
      "|    value_loss           | 9.93        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 184         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1079        |\n",
      "|    iterations           | 78          |\n",
      "|    time_elapsed         | 147         |\n",
      "|    total_timesteps      | 159744      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010229317 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.848       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.96        |\n",
      "|    n_updates            | 770         |\n",
      "|    policy_gradient_loss | -0.00025    |\n",
      "|    value_loss           | 16.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 182         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1080        |\n",
      "|    iterations           | 79          |\n",
      "|    time_elapsed         | 149         |\n",
      "|    total_timesteps      | 161792      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014045592 |\n",
      "|    clip_fraction        | 0.192       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.738       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.88        |\n",
      "|    n_updates            | 780         |\n",
      "|    policy_gradient_loss | -0.00863    |\n",
      "|    value_loss           | 15          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 184         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1080        |\n",
      "|    iterations           | 80          |\n",
      "|    time_elapsed         | 151         |\n",
      "|    total_timesteps      | 163840      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014107509 |\n",
      "|    clip_fraction        | 0.202       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.76        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.6        |\n",
      "|    n_updates            | 790         |\n",
      "|    policy_gradient_loss | -0.00193    |\n",
      "|    value_loss           | 15.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 186          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1081         |\n",
      "|    iterations           | 81           |\n",
      "|    time_elapsed         | 153          |\n",
      "|    total_timesteps      | 165888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0036064922 |\n",
      "|    clip_fraction        | 0.0189       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.16        |\n",
      "|    explained_variance   | 0.876        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.79         |\n",
      "|    n_updates            | 800          |\n",
      "|    policy_gradient_loss | -0.000569    |\n",
      "|    value_loss           | 13.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 187         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1081        |\n",
      "|    iterations           | 82          |\n",
      "|    time_elapsed         | 155         |\n",
      "|    total_timesteps      | 167936      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008484207 |\n",
      "|    clip_fraction        | 0.0823      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.26       |\n",
      "|    explained_variance   | 0.662       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.57        |\n",
      "|    n_updates            | 810         |\n",
      "|    policy_gradient_loss | -0.00427    |\n",
      "|    value_loss           | 14.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 189         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1082        |\n",
      "|    iterations           | 83          |\n",
      "|    time_elapsed         | 157         |\n",
      "|    total_timesteps      | 169984      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004558089 |\n",
      "|    clip_fraction        | 0.112       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.2        |\n",
      "|    explained_variance   | 0.899       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.94        |\n",
      "|    n_updates            | 820         |\n",
      "|    policy_gradient_loss | -0.00222    |\n",
      "|    value_loss           | 7.36        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 190         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1082        |\n",
      "|    iterations           | 84          |\n",
      "|    time_elapsed         | 158         |\n",
      "|    total_timesteps      | 172032      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007802494 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.804       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.71        |\n",
      "|    n_updates            | 830         |\n",
      "|    policy_gradient_loss | -0.004      |\n",
      "|    value_loss           | 11.7        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 190          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1081         |\n",
      "|    iterations           | 85           |\n",
      "|    time_elapsed         | 160          |\n",
      "|    total_timesteps      | 174080       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061572287 |\n",
      "|    clip_fraction        | 0.0722       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.28        |\n",
      "|    explained_variance   | 0.838        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 15.1         |\n",
      "|    n_updates            | 840          |\n",
      "|    policy_gradient_loss | -0.00117     |\n",
      "|    value_loss           | 13.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 191         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1081        |\n",
      "|    iterations           | 86          |\n",
      "|    time_elapsed         | 162         |\n",
      "|    total_timesteps      | 176128      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010108467 |\n",
      "|    clip_fraction        | 0.17        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.796       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 5.02        |\n",
      "|    n_updates            | 850         |\n",
      "|    policy_gradient_loss | -0.00279    |\n",
      "|    value_loss           | 13.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 181         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1081        |\n",
      "|    iterations           | 87          |\n",
      "|    time_elapsed         | 164         |\n",
      "|    total_timesteps      | 178176      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023404969 |\n",
      "|    clip_fraction        | 0.193       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.639       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 860         |\n",
      "|    policy_gradient_loss | -0.00188    |\n",
      "|    value_loss           | 13.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 179         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1081        |\n",
      "|    iterations           | 88          |\n",
      "|    time_elapsed         | 166         |\n",
      "|    total_timesteps      | 180224      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015057892 |\n",
      "|    clip_fraction        | 0.153       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.22       |\n",
      "|    explained_variance   | 0.135       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.79        |\n",
      "|    n_updates            | 870         |\n",
      "|    policy_gradient_loss | 5.16e-05    |\n",
      "|    value_loss           | 748         |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 180         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1079        |\n",
      "|    iterations           | 89          |\n",
      "|    time_elapsed         | 168         |\n",
      "|    total_timesteps      | 182272      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009929793 |\n",
      "|    clip_fraction        | 0.185       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.32       |\n",
      "|    explained_variance   | 0.69        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.6        |\n",
      "|    n_updates            | 880         |\n",
      "|    policy_gradient_loss | -0.00109    |\n",
      "|    value_loss           | 14.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 183         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1079        |\n",
      "|    iterations           | 90          |\n",
      "|    time_elapsed         | 170         |\n",
      "|    total_timesteps      | 184320      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008337018 |\n",
      "|    clip_fraction        | 0.0702      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.21       |\n",
      "|    explained_variance   | 0.632       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 7.5         |\n",
      "|    n_updates            | 890         |\n",
      "|    policy_gradient_loss | -0.000296   |\n",
      "|    value_loss           | 15.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 185         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1079        |\n",
      "|    iterations           | 91          |\n",
      "|    time_elapsed         | 172         |\n",
      "|    total_timesteps      | 186368      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008260571 |\n",
      "|    clip_fraction        | 0.0757      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.15       |\n",
      "|    explained_variance   | 0.576       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.51        |\n",
      "|    n_updates            | 900         |\n",
      "|    policy_gradient_loss | -0.00227    |\n",
      "|    value_loss           | 20.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 185         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1078        |\n",
      "|    iterations           | 92          |\n",
      "|    time_elapsed         | 174         |\n",
      "|    total_timesteps      | 188416      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010249307 |\n",
      "|    clip_fraction        | 0.165       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.35       |\n",
      "|    explained_variance   | 0.104       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.52        |\n",
      "|    n_updates            | 910         |\n",
      "|    policy_gradient_loss | -0.00147    |\n",
      "|    value_loss           | 19.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 500        |\n",
      "|    ep_rew_mean          | 183        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1078       |\n",
      "|    iterations           | 93         |\n",
      "|    time_elapsed         | 176        |\n",
      "|    total_timesteps      | 190464     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00819517 |\n",
      "|    clip_fraction        | 0.145      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.25      |\n",
      "|    explained_variance   | 0.778      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.89       |\n",
      "|    n_updates            | 920        |\n",
      "|    policy_gradient_loss | -0.00211   |\n",
      "|    value_loss           | 13.8       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 184          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1078         |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 178          |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0097758295 |\n",
      "|    clip_fraction        | 0.143        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.34        |\n",
      "|    explained_variance   | 0.549        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.07         |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -0.00226     |\n",
      "|    value_loss           | 13.9         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 176         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1077        |\n",
      "|    iterations           | 95          |\n",
      "|    time_elapsed         | 180         |\n",
      "|    total_timesteps      | 194560      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013826338 |\n",
      "|    clip_fraction        | 0.125       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.28       |\n",
      "|    explained_variance   | 0.0774      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.17        |\n",
      "|    n_updates            | 940         |\n",
      "|    policy_gradient_loss | -0.00197    |\n",
      "|    value_loss           | 941         |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 176          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1076         |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 182          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0050694235 |\n",
      "|    clip_fraction        | 0.0835       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.08        |\n",
      "|    explained_variance   | 0.289        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.02         |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.00131     |\n",
      "|    value_loss           | 27.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 177         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1076        |\n",
      "|    iterations           | 97          |\n",
      "|    time_elapsed         | 184         |\n",
      "|    total_timesteps      | 198656      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016874276 |\n",
      "|    clip_fraction        | 0.209       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.3        |\n",
      "|    explained_variance   | 0.854       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.2        |\n",
      "|    n_updates            | 960         |\n",
      "|    policy_gradient_loss | -0.00759    |\n",
      "|    value_loss           | 14          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 500         |\n",
      "|    ep_rew_mean          | 180         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1074        |\n",
      "|    iterations           | 98          |\n",
      "|    time_elapsed         | 186         |\n",
      "|    total_timesteps      | 200704      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012245165 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.14       |\n",
      "|    explained_variance   | 0.878       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.87        |\n",
      "|    n_updates            | 970         |\n",
      "|    policy_gradient_loss | -0.00399    |\n",
      "|    value_loss           | 14.3        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1161b170a10>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Uncomment the following line to use the PPO algorithm with the PyBulletPushEnv environment\n",
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "env = PyBulletPushEnv() # Creating the environment\n",
    "# Create the PPO model with the specified policy and environment\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "# If you wanna train the new policy on top of previous policy\n",
    "# PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_PybulletPusdEnv_TS_100000_v2\")\n",
    "# model = PPO.load(PPO_Path, env=env, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=200000)\n",
    "#\n",
    "##------------------Parallelized PPO------------------##\n",
    "# UNCOMMENT for the PPO algorithm with the PARALLELIZED PyBulletPushEnv environment\n",
    "# log_path = os.path.join(\"Training\", \"Logs\")\n",
    "# env= make_vec_env(lambda: PyBulletPushEnv(), n_envs=4)  # Create a vectorized environment\n",
    "# # Create the PPO model with the specified policy and environment\n",
    "# model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "# # If you wanna train the new policy on top of previous policy\n",
    "# # PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_PybulletPusdEnv_TS_100000_v2\")\n",
    "# # model = PPO.load(PPO_Path, env=env, tensorboard_log=log_path)\n",
    "# model.learn(total_timesteps=200000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188e6810",
   "metadata": {},
   "source": [
    "While it is training you can see the logs of training model, how well is it doing.\n",
    " - Use the below code to run on the terminal and you can visulaise it on the bowser\n",
    " ```\n",
    " tensorboard --logdir='Training\\Logs\\PPO_20'\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230e630e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Saving the Trained model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m PPO_Path = \u001b[43mos\u001b[49m.path.join(\u001b[33m\"\u001b[39m\u001b[33mTraining\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSaved Models\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mPPO_PybulletPusdEnv_TS_100000_v10\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      3\u001b[39m model.save(PPO_Path)\n",
      "\u001b[31mNameError\u001b[39m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Saving the Trained model\n",
    "# Change the version number to save the model with different version\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_PybulletPusdEnv_TS_100000_v10\")\n",
    "model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dba146c",
   "metadata": {},
   "source": [
    "#### Now you can Evelaute your model by running it without GUI and you see MEAN and STD of   cumulative rewards "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "86d89ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\UMDCP\\SEM4\\ENPM690\\Final Project\\Multi_Agent_Reinforcement_Learning\\marl-env\\Lib\\site-packages\\stable_baselines3\\common\\evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n",
      "d:\\UMDCP\\SEM4\\ENPM690\\Final Project\\Multi_Agent_Reinforcement_Learning\\marl-env\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:259: UserWarning: You tried to call render() but no `render_mode` was passed to the env constructor.\n",
      "  warnings.warn(\"You tried to call render() but no `render_mode` was passed to the env constructor.\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(220.85498148244668, 190.9591143512059)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "env=PyBulletPushEnv()\n",
    "del model # delete the model to clear memory\n",
    "# Load the model from the saved path\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_PybulletPusdEnv_TS_100000_v10\")\n",
    "model = PPO.load(PPO_Path, env=PyBulletPushEnv()) # Load the model\n",
    "evaluate_policy(model, env, n_eval_episodes=100, render=True) # Evaluate policy takes arguments as , model, env. no. of episodes to evaluate, and render mode\n",
    "# p.disconnect() # Disconnect the pybullet client\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076e3ed7",
   "metadata": {},
   "source": [
    "#### MAKE SURE you disconnect before moving to Testing the Model with GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a7a1cc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.disconnect() # Disconnect the pybullet client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94bb5df",
   "metadata": {},
   "source": [
    "### Testing a model\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b7c338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 727.2163275041408\n",
      "Episode 2 finished with score: 558.3260929952326\n",
      "Episode 3 finished with score: 498.0917226775541\n",
      "Episode 4 finished with score: 401.7045228526728\n",
      "Episode 5 finished with score: 580.3844505664163\n"
     ]
    }
   ],
   "source": [
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_PybulletPusdEnv_TS_100000_v9\")\n",
    "env=PyBulletPushEnv(render_mode=\"human\") # Creating the environment, render mode says to open GUI\n",
    "model = PPO.load(PPO_Path) # Load the model\n",
    "episode=5\n",
    "for episode in range(1, episode + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs, info = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action, _= model.predict(obs)  # Random action for demonstration\n",
    "        # print(f\"Action: {action}\")\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        # print(f\"Episode: {episode}, obs: {obs}, Done: {done}, Reward: {reward}, Truncated: {truncated}\")\n",
    "\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "\n",
    "env.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ad5383",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
