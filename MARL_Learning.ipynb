{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c22293",
   "metadata": {},
   "source": [
    "# Learning MARL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7e6d5",
   "metadata": {},
   "source": [
    "### Verifying INstrallations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641279ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pybullet as p\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print('NumPy enabled:', p.isNumpyEnabled())\n",
    "print(\"PettingZoo & Stable-Baselines3 & NumPy installed successfully!\")\n",
    "\n",
    "# Quick test: Initialize PyBullet\n",
    "physics_client = p.connect(p.DIRECT)\n",
    "p.disconnect()\n",
    "print(\"PyBullet initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff9ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from supersuit import resize_v1\n",
    "from stable_baselines3 import PPO\n",
    "import supersuit as ss\n",
    "\n",
    "# PettingZoo Environment\n",
    "env = pistonball_v6.env()  # Create AEC environment\n",
    "env = aec_to_parallel(env)  # Convert AEC to ParallelEnv\n",
    "env = resize_v1(env, x_size=84, y_size=84)  # Resize observations\n",
    "env = ss.pettingzoo_env_to_vec_env_v1(env)  # Convert to VecEnv\n",
    "env = ss.concat_vec_envs_v1(env, num_vec_envs=1, base_class='stable_baselines3')  # Reduce num_vec_envs\n",
    "\n",
    "# Initialize PPO with reduced n_steps and batch_size\n",
    "model = PPO('MlpPolicy', env, verbose=1, n_steps=512, batch_size=64)\n",
    "\n",
    "# Train for a few timesteps (testing purposes)\n",
    "model.learn(total_timesteps=5000)\n",
    "\n",
    "# Save Model\n",
    "model.save(\"ppo_pistonball\")\n",
    "\n",
    "print(\"Training complete and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e471b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from supersuit import resize_v1\n",
    "import supersuit as ss\n",
    "from stable_baselines3 import PPO\n",
    "import numpy as np\n",
    "import pygame\n",
    "import time\n",
    "\n",
    "# Initialize original PettingZoo environment (AEC)\n",
    "env = pistonball_v6.env(render_mode='human')\n",
    "env.reset()\n",
    "\n",
    "env = aec_to_parallel(env)  # Convert AEC to ParallelEnv\n",
    "env = resize_v1(env, x_size=84, y_size=84)  # Resize observations\n",
    "# Convert to Parallel, then VecEnv for compatibility\n",
    "parallel_env = ss.pettingzoo_env_to_vec_env_v1(env)\n",
    "vec_env = ss.concat_vec_envs_v1(parallel_env, num_vec_envs=1, base_class='stable_baselines3')\n",
    "\n",
    "# Load your trained model\n",
    "model = PPO.load(\"ppo_pistonball\")\n",
    "\n",
    "# Reset environment\n",
    "obs = vec_env.reset()\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    # Predict action from trained model\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    \n",
    "    # Take the predicted action\n",
    "    obs, rewards, dones, info = vec_env.step(action)\n",
    "\n",
    "    # Render the environment visually\n",
    "    # env.reset()\n",
    "    env.render()\n",
    "\n",
    "    # Check if episode is complete\n",
    "    done = np.all(dones)\n",
    "\n",
    "    # Slow down the visualization for clear observation\n",
    "    time.sleep(0.02)\n",
    "\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c940078f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding the environment, Piston Ball dataset and how has made observatiosna and rewards.\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "from pettingzoo.utils.conversions import aec_to_parallel\n",
    "from supersuit import resize_v1 \n",
    "\n",
    "\n",
    "env = pistonball_v6.parallel_env(render_mode=\"human\")\n",
    "env = resize_v1(env, x_size=84, y_size=84)  # Resize observations\n",
    "observations = env.reset()\n",
    "# Load your trained model\n",
    "\n",
    "while env.agents:\n",
    "    # action, _ = model.predict(observations, deterministic=True)\n",
    "    # this is where you would insert your policy\n",
    "    actions = {agent: env.action_space(agent).sample() for agent in env.agents}\n",
    "\n",
    "    observations, rewards, terminations, truncations, infos = env.step(actions)\n",
    "    print(\"observation\",rewards)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b6a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"type\", (observations.get('piston_1')).shape)\n",
    "print(\"type\", (observations.get('piston_1'))[83])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1c5af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "env = pistonball_v6.env(render_mode=\"human\")\n",
    "env.reset(seed=42)\n",
    "\n",
    "for agent in env.agent_iter():\n",
    "    observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "    if termination or truncation:\n",
    "        action = None\n",
    "    else:\n",
    "        # this is where you would insert your policy\n",
    "        action = env.action_space(agent).sample()\n",
    "\n",
    "    env.step(action)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3a88e9",
   "metadata": {},
   "source": [
    "### This is the example given in the Petting ZOO site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdc8069",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic code which shows what it's like to run PPO on the Pistonball env using the parallel API, this code is inspired by CleanRL.\n",
    "\n",
    "This code is exceedingly basic, with no logging or weights saving.\n",
    "The intention was for users to have a (relatively clean) ~200 line file to refer to when they want to design their own learning algorithm.\n",
    "\n",
    "Author: Jet (https://github.com/jjshoots)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from supersuit import color_reduction_v0, frame_stack_v1, resize_v1\n",
    "from torch.distributions.categorical import Categorical\n",
    "\n",
    "from pettingzoo.butterfly import pistonball_v6\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        super().__init__()\n",
    "\n",
    "        self.network = nn.Sequential(\n",
    "            self._layer_init(nn.Conv2d(4, 32, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(32, 64, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            self._layer_init(nn.Conv2d(64, 128, 3, padding=1)),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            self._layer_init(nn.Linear(128 * 8 * 8, 512)),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.actor = self._layer_init(nn.Linear(512, num_actions), std=0.01)\n",
    "        self.critic = self._layer_init(nn.Linear(512, 1))\n",
    "\n",
    "    def _layer_init(self, layer, std=np.sqrt(2), bias_const=0.0):\n",
    "        torch.nn.init.orthogonal_(layer.weight, std)\n",
    "        torch.nn.init.constant_(layer.bias, bias_const)\n",
    "        return layer\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(self.network(x / 255.0))\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        hidden = self.network(x / 255.0)\n",
    "        logits = self.actor(hidden)\n",
    "        probs = Categorical(logits=logits)\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)\n",
    "\n",
    "\n",
    "def batchify_obs(obs, device):\n",
    "    \"\"\"Converts PZ style observations to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    obs = np.stack([obs[a] for a in obs], axis=0)\n",
    "    # transpose to be (batch, channel, height, width)\n",
    "    obs = obs.transpose(0, -1, 1, 2)\n",
    "    # convert to torch\n",
    "    obs = torch.tensor(obs).to(device)\n",
    "\n",
    "    return obs\n",
    "\n",
    "\n",
    "def batchify(x, device):\n",
    "    \"\"\"Converts PZ style returns to batch of torch arrays.\"\"\"\n",
    "    # convert to list of np arrays\n",
    "    x = np.stack([x[a] for a in x], axis=0)\n",
    "    # convert to torch\n",
    "    x = torch.tensor(x).to(device)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def unbatchify(x, env):\n",
    "    \"\"\"Converts np array to PZ style arguments.\"\"\"\n",
    "    x = x.cpu().numpy()\n",
    "    x = {a: x[i] for i, a in enumerate(env.possible_agents)}\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"ALGO PARAMS\"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ent_coef = 0.1\n",
    "    vf_coef = 0.1\n",
    "    clip_coef = 0.1\n",
    "    gamma = 0.99\n",
    "    batch_size = 32\n",
    "    stack_size = 4\n",
    "    frame_size = (64, 64)\n",
    "    max_cycles = 125\n",
    "    total_episodes = 5\n",
    "\n",
    "    \"\"\" ENV SETUP \"\"\"\n",
    "    env = pistonball_v6.parallel_env(\n",
    "        render_mode=\"rgb_array\", continuous=False, max_cycles=max_cycles\n",
    "    )\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, frame_size[0], frame_size[1])\n",
    "    env = frame_stack_v1(env, stack_size=stack_size)\n",
    "    num_agents = len(env.possible_agents)\n",
    "    num_actions = env.action_space(env.possible_agents[0]).n\n",
    "    observation_size = env.observation_space(env.possible_agents[0]).shape\n",
    "\n",
    "    \"\"\" LEARNER SETUP \"\"\"\n",
    "    agent = Agent(num_actions=num_actions).to(device)\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=0.001, eps=1e-5)\n",
    "\n",
    "    \"\"\" ALGO LOGIC: EPISODE STORAGE\"\"\"\n",
    "    end_step = 0\n",
    "    total_episodic_return = 0\n",
    "    rb_obs = torch.zeros((max_cycles, num_agents, stack_size, *frame_size)).to(device)\n",
    "    rb_actions = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_logprobs = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_rewards = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_terms = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "    rb_values = torch.zeros((max_cycles, num_agents)).to(device)\n",
    "\n",
    "    \"\"\" TRAINING LOGIC \"\"\"\n",
    "    # train for n number of episodes\n",
    "    for episode in range(total_episodes):\n",
    "        # collect an episode\n",
    "        with torch.no_grad():\n",
    "            # collect observations and convert to batch of torch tensors\n",
    "            next_obs, info = env.reset(seed=None)\n",
    "            # reset the episodic return\n",
    "            total_episodic_return = 0\n",
    "\n",
    "            # each episode has num_steps\n",
    "            for step in range(0, max_cycles):\n",
    "                # rollover the observation\n",
    "                obs = batchify_obs(next_obs, device)\n",
    "\n",
    "                # get action from the agent\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "\n",
    "                # execute the environment and log data\n",
    "                next_obs, rewards, terms, truncs, infos = env.step(\n",
    "                    unbatchify(actions, env)\n",
    "                )\n",
    "\n",
    "                # add to episode storage\n",
    "                rb_obs[step] = obs\n",
    "                rb_rewards[step] = batchify(rewards, device)\n",
    "                rb_terms[step] = batchify(terms, device)\n",
    "                rb_actions[step] = actions\n",
    "                rb_logprobs[step] = logprobs\n",
    "                rb_values[step] = values.flatten()\n",
    "\n",
    "                # compute episodic return\n",
    "                total_episodic_return += rb_rewards[step].cpu().numpy()\n",
    "\n",
    "                # if we reach termination or truncation, end\n",
    "                if any([terms[a] for a in terms]) or any([truncs[a] for a in truncs]):\n",
    "                    end_step = step\n",
    "                    break\n",
    "\n",
    "        # bootstrap value if not done\n",
    "        with torch.no_grad():\n",
    "            rb_advantages = torch.zeros_like(rb_rewards).to(device)\n",
    "            for t in reversed(range(end_step)):\n",
    "                delta = (\n",
    "                    rb_rewards[t]\n",
    "                    + gamma * rb_values[t + 1] * rb_terms[t + 1]\n",
    "                    - rb_values[t]\n",
    "                )\n",
    "                rb_advantages[t] = delta + gamma * gamma * rb_advantages[t + 1]\n",
    "            rb_returns = rb_advantages + rb_values\n",
    "\n",
    "        # convert our episodes to batch of individual transitions\n",
    "        b_obs = torch.flatten(rb_obs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_logprobs = torch.flatten(rb_logprobs[:end_step], start_dim=0, end_dim=1)\n",
    "        b_actions = torch.flatten(rb_actions[:end_step], start_dim=0, end_dim=1)\n",
    "        b_returns = torch.flatten(rb_returns[:end_step], start_dim=0, end_dim=1)\n",
    "        b_values = torch.flatten(rb_values[:end_step], start_dim=0, end_dim=1)\n",
    "        b_advantages = torch.flatten(rb_advantages[:end_step], start_dim=0, end_dim=1)\n",
    "\n",
    "        # Optimizing the policy and value network\n",
    "        b_index = np.arange(len(b_obs))\n",
    "        clip_fracs = []\n",
    "        for repeat in range(3):\n",
    "            # shuffle the indices we use to access the data\n",
    "            np.random.shuffle(b_index)\n",
    "            for start in range(0, len(b_obs), batch_size):\n",
    "                # select the indices we want to train on\n",
    "                end = start + batch_size\n",
    "                batch_index = b_index[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, value = agent.get_action_and_value(\n",
    "                    b_obs[batch_index], b_actions.long()[batch_index]\n",
    "                )\n",
    "                logratio = newlogprob - b_logprobs[batch_index]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # calculate approx_kl http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clip_fracs += [\n",
    "                        ((ratio - 1.0).abs() > clip_coef).float().mean().item()\n",
    "                    ]\n",
    "\n",
    "                # normalize advantaegs\n",
    "                advantages = b_advantages[batch_index]\n",
    "                advantages = (advantages - advantages.mean()) / (\n",
    "                    advantages.std() + 1e-8\n",
    "                )\n",
    "\n",
    "                # Policy loss\n",
    "                pg_loss1 = -b_advantages[batch_index] * ratio\n",
    "                pg_loss2 = -b_advantages[batch_index] * torch.clamp(\n",
    "                    ratio, 1 - clip_coef, 1 + clip_coef\n",
    "                )\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # Value loss\n",
    "                value = value.flatten()\n",
    "                v_loss_unclipped = (value - b_returns[batch_index]) ** 2\n",
    "                v_clipped = b_values[batch_index] + torch.clamp(\n",
    "                    value - b_values[batch_index],\n",
    "                    -clip_coef,\n",
    "                    clip_coef,\n",
    "                )\n",
    "                v_loss_clipped = (v_clipped - b_returns[batch_index]) ** 2\n",
    "                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                v_loss = 0.5 * v_loss_max.mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - ent_coef * entropy_loss + v_loss * vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        print(f\"Training episode {episode}\")\n",
    "        print(f\"Episodic Return: {np.mean(total_episodic_return)}\")\n",
    "        print(f\"Episode Length: {end_step}\")\n",
    "        print(\"\")\n",
    "        print(f\"Value Loss: {v_loss.item()}\")\n",
    "        print(f\"Policy Loss: {pg_loss.item()}\")\n",
    "        print(f\"Old Approx KL: {old_approx_kl.item()}\")\n",
    "        print(f\"Approx KL: {approx_kl.item()}\")\n",
    "        print(f\"Clip Fraction: {np.mean(clip_fracs)}\")\n",
    "        print(f\"Explained Variance: {explained_var.item()}\")\n",
    "        print(\"\\n-------------------------------------------\\n\")\n",
    "\n",
    "    \"\"\" RENDER THE POLICY \"\"\"\n",
    "    env = pistonball_v6.parallel_env(render_mode=\"human\", continuous=False)\n",
    "    env = color_reduction_v0(env)\n",
    "    env = resize_v1(env, 64, 64)\n",
    "    env = frame_stack_v1(env, stack_size=4)\n",
    "\n",
    "    agent.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # render 5 episodes out\n",
    "        for episode in range(5):\n",
    "            obs, infos = env.reset(seed=None)\n",
    "            obs = batchify_obs(obs, device)\n",
    "            terms = [False]\n",
    "            truncs = [False]\n",
    "            while not any(terms) and not any(truncs):\n",
    "                actions, logprobs, _, values = agent.get_action_and_value(obs)\n",
    "                obs, rewards, terms, truncs, infos = env.step(unbatchify(actions, env))\n",
    "                obs = batchify_obs(obs, device)\n",
    "                terms = [terms[a] for a in terms]\n",
    "                truncs = [truncs[a] for a in truncs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1bfad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c78ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import time\n",
    "\n",
    "# Initialize PyBullet\n",
    "physics_client = p.connect(p.GUI)  # or p.DIRECT for non-GUI\n",
    "p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "# Setup world and physics\n",
    "p.setGravity(0, 0, -9.8)\n",
    "plane_id = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "# Load robot agents\n",
    "robot_start_positions = [[0, 0, 0.2], [1, 0, 0.2]]\n",
    "robot_ids = []\n",
    "for pos in robot_start_positions:\n",
    "    robot_id = p.loadURDF(\"r2d2.urdf\", pos)\n",
    "    robot_ids.append(robot_id)\n",
    "\n",
    "# Simulation loop (simple demo)\n",
    "for step in range(500):\n",
    "    p.stepSimulation()\n",
    "    time.sleep(1./240.)\n",
    "\n",
    "p.disconnect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ba512f",
   "metadata": {},
   "source": [
    "## Structuring Custom Envionment \n",
    "#### UsinG ChatGPT and It has many loop holes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66af8049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
