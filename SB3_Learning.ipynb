{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb456c1",
   "metadata": {},
   "source": [
    "# Learning RL using StableBaseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc02b82",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae604605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb58a9f",
   "metadata": {},
   "source": [
    "Here We are using the inbuild example environment of Cartpole environemnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2580265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the vatiable for the model name\n",
    "env_name = \"CartPole-v1\"\n",
    "#Creating the environment , remnder_mode is set to human for visualisation\n",
    "env = gym.make(env_name,  render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd7d6",
   "metadata": {},
   "source": [
    "Understanding the environment class menthods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50bb69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04225325, -0.03066197, -0.03478266,  0.04284509], dtype=float32), {})\n",
      "Discrete(2)\n",
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "obs :  [ 0.04164001  0.16494104 -0.03392576 -0.26060602] reward :  1.0 Done :  False info :  False\n"
     ]
    }
   ],
   "source": [
    "#Checking what are the outputs from the resetting the environment\n",
    "print(env.reset())\n",
    "# Checking the action and observation space of the environment\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "#Observing the output of the step function with the action 1\n",
    "obs, reward, done, info, _ = env.step(1)\n",
    "print(\"obs : \", obs,\"reward : \", reward,\"Done : \" ,done,\"info : \" ,info)\n",
    "# closing the environment to close the Visualisation window\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a139b4",
   "metadata": {},
   "source": [
    "Now Lets run the episode of balacing the cartpole \n",
    "and assuming we are not learning or trying to learn to balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ec0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 16.0\n",
      "Episode 2 finished with score: 19.0\n",
      "Episode 3 finished with score: 23.0\n",
      "Episode 4 finished with score: 38.0\n",
      "Episode 5 finished with score: 20.0\n",
      "Episode 6 finished with score: 17.0\n",
      "Episode 7 finished with score: 29.0\n",
      "Episode 8 finished with score: 26.0\n",
      "Episode 9 finished with score: 53.0\n",
      "Episode 10 finished with score: 21.0\n"
     ]
    }
   ],
   "source": [
    "# Lets define the number of episodes that is\n",
    "# we will run the simulation for number of episodes time just \n",
    "# to see how the environment is working and how the action space is being used\n",
    "# and how the reward is being calculated\n",
    "episodes = 10 # Defineing the number of episodes\n",
    "for episode in range(1, episodes + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action= env.action_space.sample()  # Random action for demonstration\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "\n",
    "env.close()  # Close the environment after all episodes are done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5de812",
   "metadata": {},
   "source": [
    "So in above logs you can see the random score being accumulated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869c0f3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f26a1",
   "metadata": {},
   "source": [
    "Training the model to select action based on obs env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53b0ce",
   "metadata": {},
   "source": [
    "Before starting the training, we need to create a directory for logging\n",
    "the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a778383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directory and make a variable of that directory\n",
    "# Log path of the taraing\n",
    "log_path = os.path.join(\"Training\", \"Logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b58aa6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06172d",
   "metadata": {},
   "source": [
    "Again recreating the env variable,\n",
    "Making a dummy vec env for vectorizing a single env\n",
    "and setting up the model with PPO, env and saving the logs in log path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(env_name, render_mode=\"human\") # redefining the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "# Create the PPO model with the specified policy and environment\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c521c2",
   "metadata": {},
   "source": [
    "Begin learning using the below command and set a total timesteps to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b626dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1056 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 906         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008801829 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.0057      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.2         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 52.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1015        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009886082 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1080       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00847789 |\n",
      "|    clip_fraction        | 0.081      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22         |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    value_loss           | 52         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1127       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01121307 |\n",
      "|    clip_fraction        | 0.0951     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | 0.319      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.8       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 61         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1149       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00565252 |\n",
      "|    clip_fraction        | 0.0357     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.3       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    value_loss           | 58.5       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1165         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075403545 |\n",
      "|    clip_fraction        | 0.0859       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.55         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 35.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1186        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005936921 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1195        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010442162 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.72        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 25.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1198        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005441611 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2ab898c6710>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734900cb",
   "metadata": {},
   "source": [
    "Next task is to save the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create the directory and variable of the path to the directory\n",
    "# Save the PPO model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20154f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using this command\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5c867",
   "metadata": {},
   "source": [
    "Overall Consolidated Training a model Looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole\")\n",
    "env=gym.make(env_name, render_mode=\"human\") # redefining the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)# Create the PPO model with the specified policy and environment\n",
    "model.learn(total_timesteps=20000)\n",
    "model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55704781",
   "metadata": {},
   "source": [
    "Now you can delete the variable model so that I can demonstrate howe to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # Delete the model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48d6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Load the model using the same path as above\n",
    "# Load the PPO model\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d11b9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b075e",
   "metadata": {},
   "source": [
    "Once the model is loaded now you can evaluate the policy using another method as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5ef20",
   "metadata": {},
   "source": [
    "This evaluation is done based on given num random episodes and thier scores are averaged out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39072034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate policy takes arguments as , model, env. no. of episodes to evaluate, and render mode\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bbf5e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531a225",
   "metadata": {},
   "source": [
    "While Evalute policy doesnot give you enough insites of each episodes, we create Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 279.0\n",
      "Episode 2 finished with score: 1615.0\n",
      "Episode 3 finished with score: 759.0\n",
      "Episode 4 finished with score: 729.0\n",
      "Episode 5 finished with score: 96.0\n",
      "Episode 6 finished with score: 419.0\n",
      "Episode 7 finished with score: 828.0\n",
      "Episode 8 finished with score: 176.0\n",
      "Episode 9 finished with score: 86.0\n",
      "Episode 10 finished with score: 188.0\n"
     ]
    }
   ],
   "source": [
    "# Now all the code is same as above but we will use\n",
    "# trained model to predict the action and then take a step in the environment\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs ,info= env.reset()\n",
    "    done = False\n",
    "    score=0\n",
    "\n",
    "    while not done:\n",
    "        # olnly change this line to use the trained model to predict the action\n",
    "        action,_= model.predict(obs)\n",
    "        # action= env.action_space.sample()  # Random action for demonstration\n",
    "        obs, reward, done, info,_ = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b535397",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
