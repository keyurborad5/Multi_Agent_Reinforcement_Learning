{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb456c1",
   "metadata": {},
   "source": [
    "# Learning RL using StableBaseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc02b82",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae604605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb58a9f",
   "metadata": {},
   "source": [
    "Here We are using the inbuild example environment of Cartpole environemnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2580265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the vatiable for the model name\n",
    "env_name = \"CartPole-v1\"\n",
    "#Creating the environment , remnder_mode is set to human for visualisation\n",
    "env = gym.make(env_name,  render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd7d6",
   "metadata": {},
   "source": [
    "Understanding the environment class menthods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50bb69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04225325, -0.03066197, -0.03478266,  0.04284509], dtype=float32), {})\n",
      "Discrete(2)\n",
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "obs :  [ 0.04164001  0.16494104 -0.03392576 -0.26060602] reward :  1.0 Done :  False info :  False\n"
     ]
    }
   ],
   "source": [
    "#Checking what are the outputs from the resetting the environment\n",
    "print(env.reset())\n",
    "# Checking the action and observation space of the environment\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "#Observing the output of the step function with the action 1\n",
    "obs, reward, done, info, _ = env.step(1)\n",
    "print(\"obs : \", obs,\"reward : \", reward,\"Done : \" ,done,\"info : \" ,info)\n",
    "# closing the environment to close the Visualisation window\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a139b4",
   "metadata": {},
   "source": [
    "Now Lets run the episode of balacing the cartpole \n",
    "and assuming we are not learning or trying to learn to balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5ec0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 16.0\n",
      "Episode 2 finished with score: 19.0\n",
      "Episode 3 finished with score: 23.0\n",
      "Episode 4 finished with score: 38.0\n",
      "Episode 5 finished with score: 20.0\n",
      "Episode 6 finished with score: 17.0\n",
      "Episode 7 finished with score: 29.0\n",
      "Episode 8 finished with score: 26.0\n",
      "Episode 9 finished with score: 53.0\n",
      "Episode 10 finished with score: 21.0\n"
     ]
    }
   ],
   "source": [
    "# Lets define the number of episodes that is\n",
    "# we will run the simulation for number of episodes time just \n",
    "# to see how the environment is working and how the action space is being used\n",
    "# and how the reward is being calculated\n",
    "episodes = 10 # Defineing the number of episodes\n",
    "for episode in range(1, episodes + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action= env.action_space.sample()  # Random action for demonstration\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "\n",
    "env.close()  # Close the environment after all episodes are done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5de812",
   "metadata": {},
   "source": [
    "So in above logs you can see the random score being accumulated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869c0f3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f26a1",
   "metadata": {},
   "source": [
    "Training the model to select action based on obs env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53b0ce",
   "metadata": {},
   "source": [
    "Before starting the training, we need to create a directory for logging\n",
    "the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a778383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directory and make a variable of that directory\n",
    "# Log path of the taraing\n",
    "log_path = os.path.join(\"Training\", \"Logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b58aa6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06172d",
   "metadata": {},
   "source": [
    "Again recreating the env variable,\n",
    "Making a dummy vec env for vectorizing a single env\n",
    "and setting up the model with PPO, env and saving the logs in log path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06acba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(env_name, render_mode=\"human\") # redefining the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "# Create the PPO model with the specified policy and environment\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c521c2",
   "metadata": {},
   "source": [
    "Begin learning using the below command and set a total timesteps to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b626dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1056 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 906         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008801829 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.0057      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.2         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 52.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1015        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009886082 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1080       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00847789 |\n",
      "|    clip_fraction        | 0.081      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22         |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    value_loss           | 52         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1127       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01121307 |\n",
      "|    clip_fraction        | 0.0951     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | 0.319      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.8       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 61         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1149       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00565252 |\n",
      "|    clip_fraction        | 0.0357     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.3       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    value_loss           | 58.5       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1165         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075403545 |\n",
      "|    clip_fraction        | 0.0859       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.55         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 35.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1186        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005936921 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1195        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010442162 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.72        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 25.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1198        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005441611 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2ab898c6710>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734900cb",
   "metadata": {},
   "source": [
    "Next task is to save the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create the directory and variable of the path to the directory\n",
    "# Save the PPO model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20154f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using this command\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5c867",
   "metadata": {},
   "source": [
    "Overall Consolidated Training a model Looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole\")\n",
    "env=gym.make(env_name, render_mode=\"human\") # redefining the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)# Create the PPO model with the specified policy and environment\n",
    "model.learn(total_timesteps=20000)\n",
    "model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55704781",
   "metadata": {},
   "source": [
    "Now you can delete the variable model so that I can demonstrate howe to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # Delete the model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48d6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Load the model using the same path as above\n",
    "# Load the PPO model\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d11b9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b075e",
   "metadata": {},
   "source": [
    "Once the model is loaded now you can evaluate the policy using another method as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5ef20",
   "metadata": {},
   "source": [
    "This evaluation is done based on given num random episodes and thier scores are averaged out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39072034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate policy takes arguments as , model, env. no. of episodes to evaluate, and render mode\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bbf5e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531a225",
   "metadata": {},
   "source": [
    "While Evalute policy doesnot give you enough insites of each episodes, we create Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806d7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 279.0\n",
      "Episode 2 finished with score: 1615.0\n",
      "Episode 3 finished with score: 759.0\n",
      "Episode 4 finished with score: 729.0\n",
      "Episode 5 finished with score: 96.0\n",
      "Episode 6 finished with score: 419.0\n",
      "Episode 7 finished with score: 828.0\n",
      "Episode 8 finished with score: 176.0\n",
      "Episode 9 finished with score: 86.0\n",
      "Episode 10 finished with score: 188.0\n"
     ]
    }
   ],
   "source": [
    "# Now all the code is same as above but we will use\n",
    "# trained model to predict the action and then take a step in the environment\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs ,info= env.reset()\n",
    "    done = False\n",
    "    score=0\n",
    "\n",
    "    while not done:\n",
    "        # olnly change this line to use the trained model to predict the action\n",
    "        action,_= model.predict(obs)\n",
    "        # action= env.action_space.sample()  # Random action for demonstration\n",
    "        obs, reward, done, info,_ = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b535397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0d6a2d",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09afb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56bb1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "\n",
    "# env = gym.make_vec(env_name,num_envs=4, render_mode=\"human\", vectorization_mode=\"vector_entry_point\") # redefining the environment\n",
    "# env= VecEnv(env, num_envs=4) # Wrapping the environment in a VecFrameStack for stacking frames\n",
    "# env = VecEnv(lambda: gym.make(\"CartPole-v1\"), n_envs=4)\n",
    "env = make_vec_env(lambda: gym.make(\"CartPole-v1\"), n_envs=4)\n",
    "\n",
    "# model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path) # Create the PPO model with the specified policy and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c71767f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x1fdaf94a8d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "304fa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\Logs\\PPO_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2815     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.2        |\n",
      "|    ep_rew_mean          | 32.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1059        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014833221 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | -0.00338    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3           |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59.8        |\n",
      "|    ep_rew_mean          | 59.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018066492 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013797177 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 40.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 158         |\n",
      "|    ep_rew_mean          | 158         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011375943 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.527       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 215         |\n",
      "|    ep_rew_mean          | 215         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011790767 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.44        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 24          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 266         |\n",
      "|    ep_rew_mean          | 266         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 722         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009408331 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 329        |\n",
      "|    ep_rew_mean          | 329        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01087546 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.513     |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.39       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 10.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 386         |\n",
      "|    ep_rew_mean          | 386         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 706         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007141512 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 429          |\n",
      "|    ep_rew_mean          | 429          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 702          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024460864 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.142        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 1.4          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 467          |\n",
      "|    ep_rew_mean          | 467          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 699          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012298279 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.487       |\n",
      "|    explained_variance   | 0.0397       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -8.31e-05    |\n",
      "|    value_loss           | 1.14         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 491          |\n",
      "|    ep_rew_mean          | 491          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 601          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 163          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033676992 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0628       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0359       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 0.766        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019091431 |\n",
      "|    clip_fraction        | 0.0144       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.0485       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0858       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    value_loss           | 0.467        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fdaf82be90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78578e74",
   "metadata": {},
   "source": [
    "### Testing model after Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac7b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create the directory and variable of the path to the directory\n",
    "# Save the PPO model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole_TS_1000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a0b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using this command\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7bc6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the vatiable for the model name\n",
    "env_name = \"CartPole-v1\"\n",
    "#Creating the environment , remnder_mode is set to human for visualisation\n",
    "env = gym.make(env_name,  render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d0cd35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "del model  # Delete the model to demonstrate loading\n",
    "# Load the model using the same path as above\n",
    "# Load the PPO model\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ea824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 60.0\n",
      "Episode 2 finished with score: 60.0\n",
      "Episode 3 finished with score: 60.0\n",
      "Episode 4 finished with score: 60.0\n",
      "Episode 5 finished with score: 60.0\n",
      "Episode 6 finished with score: 60.0\n",
      "Episode 7 finished with score: 60.0\n",
      "Episode 8 finished with score: 60.0\n",
      "Episode 9 finished with score: 60.0\n",
      "Episode 10 finished with score: 60.0\n"
     ]
    }
   ],
   "source": [
    "# Now all the code is same as above but we will use\n",
    "# trained model to predict the action and then take a step in the environment\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs ,info= env.reset()\n",
    "    done = False\n",
    "    score=0\n",
    "\n",
    "    while (not done and score<60):#adding extra condition to stop the simualtion\n",
    "        # olnly change this line to use the trained model to predict the action\n",
    "        action,_= model.predict(obs)\n",
    "        # action= env.action_space.sample()  # Random action for demonstration\n",
    "        obs, reward, done, info,_ = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6605a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
