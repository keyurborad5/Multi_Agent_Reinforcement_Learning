{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb456c1",
   "metadata": {},
   "source": [
    "# Learning RL using StableBaseline3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc02b82",
   "metadata": {},
   "source": [
    "### Importing Necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae604605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb58a9f",
   "metadata": {},
   "source": [
    "Here We are using the inbuild example environment of Cartpole environemnt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f2580265",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the vatiable for the model name\n",
    "env_name = \"CartPole-v1\"\n",
    "#Creating the environment , remnder_mode is set to human for visualisation\n",
    "env = gym.make(env_name,  render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86cd7d6",
   "metadata": {},
   "source": [
    "Understanding the environment class menthods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "50bb69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04225325, -0.03066197, -0.03478266,  0.04284509], dtype=float32), {})\n",
      "Discrete(2)\n",
      "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)\n",
      "obs :  [ 0.04164001  0.16494104 -0.03392576 -0.26060602] reward :  1.0 Done :  False info :  False\n"
     ]
    }
   ],
   "source": [
    "#Checking what are the outputs from the resetting the environment\n",
    "print(env.reset())\n",
    "# Checking the action and observation space of the environment\n",
    "print(env.action_space)\n",
    "print(env.observation_space)\n",
    "#Observing the output of the step function with the action 1\n",
    "obs, reward, done, info, _ = env.step(1)\n",
    "print(\"obs : \", obs,\"reward : \", reward,\"Done : \" ,done,\"info : \" ,info)\n",
    "# closing the environment to close the Visualisation window\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a139b4",
   "metadata": {},
   "source": [
    "Now Lets run the episode of balacing the cartpole \n",
    "and assuming we are not learning or trying to learn to balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6a5ec0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 16.0\n",
      "Episode 2 finished with score: 19.0\n",
      "Episode 3 finished with score: 23.0\n",
      "Episode 4 finished with score: 38.0\n",
      "Episode 5 finished with score: 20.0\n",
      "Episode 6 finished with score: 17.0\n",
      "Episode 7 finished with score: 29.0\n",
      "Episode 8 finished with score: 26.0\n",
      "Episode 9 finished with score: 53.0\n",
      "Episode 10 finished with score: 21.0\n"
     ]
    }
   ],
   "source": [
    "# Lets define the number of episodes that is\n",
    "# we will run the simulation for number of episodes time just \n",
    "# to see how the environment is working and how the action space is being used\n",
    "# and how the reward is being calculated\n",
    "episodes = 10 # Defineing the number of episodes\n",
    "for episode in range(1, episodes + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action= env.action_space.sample()  # Random action for demonstration\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, info, _ = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "\n",
    "env.close()  # Close the environment after all episodes are done\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5de812",
   "metadata": {},
   "source": [
    "So in above logs you can see the random score being accumulated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869c0f3",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944f26a1",
   "metadata": {},
   "source": [
    "Training the model to select action based on obs env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e53b0ce",
   "metadata": {},
   "source": [
    "Before starting the training, we need to create a directory for logging\n",
    "the updated weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a778383",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create directory and make a variable of that directory\n",
    "# Log path of the taraing\n",
    "log_path = os.path.join(\"Training\", \"Logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b58aa6f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb06172d",
   "metadata": {},
   "source": [
    "Again recreating the env variable,\n",
    "Making a dummy vec env for vectorizing a single env\n",
    "and setting up the model with PPO, env and saving the logs in log path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06acba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "env=gym.make(env_name, render_mode=\"human\") # redefining the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "# Create the PPO model with the specified policy and environment\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c521c2",
   "metadata": {},
   "source": [
    "Begin learning using the below command and set a total timesteps to terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b626dcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 1056 |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 1    |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 906         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 4           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008801829 |\n",
      "|    clip_fraction        | 0.11        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | 0.0057      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.2         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0165     |\n",
      "|    value_loss           | 52.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1015        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 6           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009886082 |\n",
      "|    clip_fraction        | 0.0647      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.665      |\n",
      "|    explained_variance   | 0.0924      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0177     |\n",
      "|    value_loss           | 33.5        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1080       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 7          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00847789 |\n",
      "|    clip_fraction        | 0.081      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | 0.291      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 22         |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0179    |\n",
      "|    value_loss           | 52         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1127       |\n",
      "|    iterations           | 5          |\n",
      "|    time_elapsed         | 9          |\n",
      "|    total_timesteps      | 10240      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01121307 |\n",
      "|    clip_fraction        | 0.0951     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.61      |\n",
      "|    explained_variance   | 0.319      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.8       |\n",
      "|    n_updates            | 40         |\n",
      "|    policy_gradient_loss | -0.0199    |\n",
      "|    value_loss           | 61         |\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "| time/                   |            |\n",
      "|    fps                  | 1149       |\n",
      "|    iterations           | 6          |\n",
      "|    time_elapsed         | 10         |\n",
      "|    total_timesteps      | 12288      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00565252 |\n",
      "|    clip_fraction        | 0.0357     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.597     |\n",
      "|    explained_variance   | 0.41       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 16.3       |\n",
      "|    n_updates            | 50         |\n",
      "|    policy_gradient_loss | -0.0103    |\n",
      "|    value_loss           | 58.5       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 1165         |\n",
      "|    iterations           | 7            |\n",
      "|    time_elapsed         | 12           |\n",
      "|    total_timesteps      | 14336        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0075403545 |\n",
      "|    clip_fraction        | 0.0859       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.605       |\n",
      "|    explained_variance   | 0.828        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.55         |\n",
      "|    n_updates            | 60           |\n",
      "|    policy_gradient_loss | -0.0114      |\n",
      "|    value_loss           | 35.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1186        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 13          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005936921 |\n",
      "|    clip_fraction        | 0.0564      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.593      |\n",
      "|    explained_variance   | 0.645       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 47.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.0113     |\n",
      "|    value_loss           | 59          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1195        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010442162 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.598      |\n",
      "|    explained_variance   | 0.884       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.72        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 25.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 1198        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 20480       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005441611 |\n",
      "|    clip_fraction        | 0.0771      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.589      |\n",
      "|    explained_variance   | 0.626       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.5        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.00905    |\n",
      "|    value_loss           | 35.8        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2ab898c6710>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "734900cb",
   "metadata": {},
   "source": [
    "Next task is to save the model,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ed04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create the directory and variable of the path to the directory\n",
    "# Save the PPO model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20154f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using this command\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5c867",
   "metadata": {},
   "source": [
    "Overall Consolidated Training a model Looks like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e26331e",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole\")\n",
    "env=gym.make(env_name, render_mode=\"human\") # redefining the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)# Create the PPO model with the specified policy and environment\n",
    "model.learn(total_timesteps=20000)\n",
    "model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55704781",
   "metadata": {},
   "source": [
    "Now you can delete the variable model so that I can demonstrate howe to load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19a8ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # Delete the model to demonstrate loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c48d6a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Load the model using the same path as above\n",
    "# Load the PPO model\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96d11b9",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b075e",
   "metadata": {},
   "source": [
    "Once the model is loaded now you can evaluate the policy using another method as shown below"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d5ef20",
   "metadata": {},
   "source": [
    "This evaluation is done based on given num random episodes and thier scores are averaged out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "39072034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500.0, 0.0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate policy takes arguments as , model, env. no. of episodes to evaluate, and render mode\n",
    "evaluate_policy(model, env, n_eval_episodes=10, render=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108bbf5e",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6531a225",
   "metadata": {},
   "source": [
    "While Evalute policy doesnot give you enough insites of each episodes, we create Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "806d7b15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 279.0\n",
      "Episode 2 finished with score: 1615.0\n",
      "Episode 3 finished with score: 759.0\n",
      "Episode 4 finished with score: 729.0\n",
      "Episode 5 finished with score: 96.0\n",
      "Episode 6 finished with score: 419.0\n",
      "Episode 7 finished with score: 828.0\n",
      "Episode 8 finished with score: 176.0\n",
      "Episode 9 finished with score: 86.0\n",
      "Episode 10 finished with score: 188.0\n"
     ]
    }
   ],
   "source": [
    "# Now all the code is same as above but we will use\n",
    "# trained model to predict the action and then take a step in the environment\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs ,info= env.reset()\n",
    "    done = False\n",
    "    score=0\n",
    "\n",
    "    while not done:\n",
    "        # olnly change this line to use the trained model to predict the action\n",
    "        action,_= model.predict(obs)\n",
    "        # action= env.action_space.sample()  # Random action for demonstration\n",
    "        obs, reward, done, info,_ = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b535397",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea0d6a2d",
   "metadata": {},
   "source": [
    "# Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "09afb949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "56bb1415",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "\n",
    "# env = gym.make_vec(env_name,num_envs=4, render_mode=\"human\", vectorization_mode=\"vector_entry_point\") # redefining the environment\n",
    "# env= VecEnv(env, num_envs=4) # Wrapping the environment in a VecFrameStack for stacking frames\n",
    "# env = VecEnv(lambda: gym.make(\"CartPole-v1\"), n_envs=4)\n",
    "env = make_vec_env(lambda: gym.make(\"CartPole-v1\"), n_envs=4)\n",
    "\n",
    "# model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path) # Create the PPO model with the specified policy and environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c71767f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv at 0x1fdaf94a8d0>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "304fa181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\Logs\\PPO_4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 22.6     |\n",
      "|    ep_rew_mean     | 22.6     |\n",
      "| time/              |          |\n",
      "|    fps             | 2815     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 2        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 32.2        |\n",
      "|    ep_rew_mean          | 32.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1059        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014833221 |\n",
      "|    clip_fraction        | 0.218       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.682      |\n",
      "|    explained_variance   | -0.00338    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 3           |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0245     |\n",
      "|    value_loss           | 19.4        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 59.8        |\n",
      "|    ep_rew_mean          | 59.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 862         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018066492 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.645      |\n",
      "|    explained_variance   | 0.406       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0356     |\n",
      "|    value_loss           | 26.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 102         |\n",
      "|    ep_rew_mean          | 102         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 778         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013797177 |\n",
      "|    clip_fraction        | 0.176       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.603      |\n",
      "|    explained_variance   | 0.434       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.3        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0293     |\n",
      "|    value_loss           | 40.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 158         |\n",
      "|    ep_rew_mean          | 158         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 744         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 55          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011375943 |\n",
      "|    clip_fraction        | 0.148       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.573      |\n",
      "|    explained_variance   | 0.527       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 17.6        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0228     |\n",
      "|    value_loss           | 38          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 215         |\n",
      "|    ep_rew_mean          | 215         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 730         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 67          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011790767 |\n",
      "|    clip_fraction        | 0.157       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.557      |\n",
      "|    explained_variance   | 0.712       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.44        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 24          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 266         |\n",
      "|    ep_rew_mean          | 266         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 722         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 79          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009408331 |\n",
      "|    clip_fraction        | 0.126       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.536      |\n",
      "|    explained_variance   | 0.77        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.73        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0147     |\n",
      "|    value_loss           | 16.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 329        |\n",
      "|    ep_rew_mean          | 329        |\n",
      "| time/                   |            |\n",
      "|    fps                  | 718        |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 91         |\n",
      "|    total_timesteps      | 65536      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01087546 |\n",
      "|    clip_fraction        | 0.178      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.513     |\n",
      "|    explained_variance   | 0.857      |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 2.39       |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0132    |\n",
      "|    value_loss           | 10.2       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 386         |\n",
      "|    ep_rew_mean          | 386         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 706         |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 104         |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007141512 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.483      |\n",
      "|    explained_variance   | 0.889       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 0.156       |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00616    |\n",
      "|    value_loss           | 1.81        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 429          |\n",
      "|    ep_rew_mean          | 429          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 702          |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 116          |\n",
      "|    total_timesteps      | 81920        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0024460864 |\n",
      "|    clip_fraction        | 0.0297       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.484       |\n",
      "|    explained_variance   | 0.34         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.142        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.0013      |\n",
      "|    value_loss           | 1.4          |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 467          |\n",
      "|    ep_rew_mean          | 467          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 699          |\n",
      "|    iterations           | 11           |\n",
      "|    time_elapsed         | 128          |\n",
      "|    total_timesteps      | 90112        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0012298279 |\n",
      "|    clip_fraction        | 0.0118       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.487       |\n",
      "|    explained_variance   | 0.0397       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.133        |\n",
      "|    n_updates            | 100          |\n",
      "|    policy_gradient_loss | -8.31e-05    |\n",
      "|    value_loss           | 1.14         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 491          |\n",
      "|    ep_rew_mean          | 491          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 601          |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 163          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0033676992 |\n",
      "|    clip_fraction        | 0.0349       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.457       |\n",
      "|    explained_variance   | 0.0628       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0359       |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.00138     |\n",
      "|    value_loss           | 0.766        |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 500          |\n",
      "|    ep_rew_mean          | 500          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 605          |\n",
      "|    iterations           | 13           |\n",
      "|    time_elapsed         | 175          |\n",
      "|    total_timesteps      | 106496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0019091431 |\n",
      "|    clip_fraction        | 0.0144       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.447       |\n",
      "|    explained_variance   | 0.0485       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.0858       |\n",
      "|    n_updates            | 120          |\n",
      "|    policy_gradient_loss | -0.000318    |\n",
      "|    value_loss           | 0.467        |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fdaf82be90>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78578e74",
   "metadata": {},
   "source": [
    "### Testing model after Vectorization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1ac7b2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly create the directory and variable of the path to the directory\n",
    "# Save the PPO model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_CartPole_TS_1000000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7a0b415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model using this command\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b7bc6f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Making the vatiable for the model name\n",
    "env_name = \"CartPole-v1\"\n",
    "#Creating the environment , remnder_mode is set to human for visualisation\n",
    "env = gym.make(env_name,  render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6d0cd35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "del model  # Delete the model to demonstrate loading\n",
    "# Load the model using the same path as above\n",
    "# Load the PPO model\n",
    "model = PPO.load(PPO_Path, env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8f6ea824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 60.0\n",
      "Episode 2 finished with score: 60.0\n",
      "Episode 3 finished with score: 60.0\n",
      "Episode 4 finished with score: 60.0\n",
      "Episode 5 finished with score: 60.0\n",
      "Episode 6 finished with score: 60.0\n",
      "Episode 7 finished with score: 60.0\n",
      "Episode 8 finished with score: 60.0\n",
      "Episode 9 finished with score: 60.0\n",
      "Episode 10 finished with score: 60.0\n"
     ]
    }
   ],
   "source": [
    "# Now all the code is same as above but we will use\n",
    "# trained model to predict the action and then take a step in the environment\n",
    "episodes = 10\n",
    "for episode in range(1, episodes + 1):\n",
    "    obs ,info= env.reset()\n",
    "    done = False\n",
    "    score=0\n",
    "\n",
    "    while (not done and score<60):#adding extra condition to stop the simualtion\n",
    "        # olnly change this line to use the trained model to predict the action\n",
    "        action,_= model.predict(obs)\n",
    "        # action= env.action_space.sample()  # Random action for demonstration\n",
    "        obs, reward, done, info,_ = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c6605a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f64c0f9f",
   "metadata": {},
   "source": [
    "# Custom Environement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f3296d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.spaces import Discrete, Box, Dict,Tuple, MultiDiscrete, MultiBinary\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "import os\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e266b55",
   "metadata": {},
   "source": [
    "### Types of spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "64ee8cc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "3\n",
      "2\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):# Random number between 1 and 100\n",
    "    print(Discrete(5).sample()) # Discrete action space with 5 actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9bf037e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.65011007, 0.0919581 , 0.5091919 ], dtype=float32)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Box(0,1, shape=(3,), dtype=np.float32).sample() # Box action space with 3 actions and range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "173cd7f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, array([0.07912254, 0.1186677 , 0.78638875], dtype=float32))"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tuple((Discrete(5), Box(0,1, shape=(3,), dtype=np.float32))).sample() # Tuple action space with 5 actions and range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "92166d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 3,\n",
       " 'observation': array([0.11112826, 0.10480467, 0.94225425], dtype=float32)}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dict({\"action\": Discrete(5), \"observation\": Box(0,1, shape=(3,), dtype=np.float32)}).sample() # Dict action space with 5 actions and range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "53cb7686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 1, 1, 0], dtype=int8)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MultiBinary(5).sample() # MultiBinary action space with 5 actions and range between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f305b256",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 3]\n",
      "[1 4 2]\n",
      "[4 2 4]\n",
      "[0 1 2]\n",
      "[0 4 0]\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):# Random number between 1 and 100\n",
    "    print(MultiDiscrete([5, 5, 5]).sample()) # MultiDiscrete action space with 5 actions and range between 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d2528b",
   "metadata": {},
   "source": [
    "### Building env\n",
    "- BUild an agent to give best shower\n",
    "- random temp\n",
    "- 37 and 39 degreees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f78984",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShowerEnv(Env):\n",
    "    def __init__(self):\n",
    "        super(ShowerEnv, self).__init__()\n",
    "\n",
    "        self.action_space = Discrete(3)\n",
    "        # Both are same observation space\n",
    "        self.observation_space = Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)\n",
    "        # self.observation_space = Box(low=0, high=0, shape=(1,) ,dtype=np.float32)\n",
    "\n",
    "        self.state = 38 + random.randint(90, 100)  # Initial temperature (random between 38 and 40)\n",
    "        self.shower_length = 60  # Length of the shower in seconds\n",
    "    def step(self, action):\n",
    "        # Apply action on the state\n",
    "        self.state += action-1\n",
    "\n",
    "        # Decrease the shower length by 1 second\n",
    "        self.shower_length -= 1\n",
    "\n",
    "        #Reward calculation\n",
    "        if self.state>=37 and self.state<=39:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        \n",
    "        #Termination condition\n",
    "        if self.shower_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        info = {}\n",
    "        truncated = False\n",
    "        return self.state, reward, done, truncated, info\n",
    "    \n",
    "    def seed(self, seed=None):\n",
    "        np.random.seed(seed)\n",
    "        random.seed(seed)\n",
    "        \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def reset(self,*, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if seed is not None:\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "        # Reset the state to a random temperature\n",
    "        self.state =np.array([38+random.randint(-10, 10)]).astype(np.float32)\n",
    "        self.shower_length = 60\n",
    "        # Return the initial state\n",
    "        return self.state , {}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a26e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space1 = Box(low=np.array([0]), high=np.array([100]), dtype=np.float32)\n",
    "observation_space2 = Box(low=0, high=100, shape=(1,) ,dtype=np.float32)\n",
    "\n",
    "print(observation_space1.sample()) # Sample from the observation space\n",
    "print(observation_space2.sample()) # Sample from the observation space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "cb110a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(0.0, 100.0, (1,), float32)\n",
      "Discrete(3)\n",
      "[35.]\n",
      "(array([35.], dtype=float32), -1, False, False, {})\n"
     ]
    }
   ],
   "source": [
    "env = ShowerEnv() # Creating the environment\n",
    "print(env.observation_space)\n",
    "print(env.action_space) # Printing the action space of the environment\n",
    "env.reset() # Resetting the environment\n",
    "print(env.state)\n",
    "print(env.step(1)) # Taking a step in the environment with action 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb43954c",
   "metadata": {},
   "source": [
    "### Test Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "361c0930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: -16\n",
      "Episode 2 finished with score: 38\n",
      "Episode 3 finished with score: 6\n",
      "Episode 4 finished with score: 30\n",
      "Episode 5 finished with score: -54\n"
     ]
    }
   ],
   "source": [
    "episode=5\n",
    "for episode in range(1, episode + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs, info = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action= env.action_space.sample()  # Random action for demonstration\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2577589",
   "metadata": {},
   "source": [
    "### Trainning a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "aaaed1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to Training\\Logs\\PPO_7\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | -30.5    |\n",
      "| time/              |          |\n",
      "|    fps             | 3191     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -30.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1908        |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 2           |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008185738 |\n",
      "|    clip_fraction        | 0.0448      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000142   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 26          |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.00422    |\n",
      "|    value_loss           | 51.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -29.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1705        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 3           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010845172 |\n",
      "|    clip_fraction        | 0.0386      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.00173    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27          |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00497    |\n",
      "|    value_loss           | 56.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -28         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1590        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 5           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012544146 |\n",
      "|    clip_fraction        | 0.0813      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.1        |\n",
      "|    explained_variance   | -6.45e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.2        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00753    |\n",
      "|    value_loss           | 67.4        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -25.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1540         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 6            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0006822519 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -7.06e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.2         |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | 0.000275     |\n",
      "|    value_loss           | 69.5         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -28.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1500        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 8           |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009796281 |\n",
      "|    clip_fraction        | 0.0941      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | -6.43e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.8        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00752    |\n",
      "|    value_loss           | 70.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -28.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1482        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012721714 |\n",
      "|    clip_fraction        | 0.135       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -0.000643   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 41.3        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0111     |\n",
      "|    value_loss           | 67.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -28.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1470        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 11          |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005322897 |\n",
      "|    clip_fraction        | 0.0216      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | -9.92e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.2        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.000674   |\n",
      "|    value_loss           | 74.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -25.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1462        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 12          |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012467993 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.08       |\n",
      "|    explained_variance   | 0.00035     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.5        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00113    |\n",
      "|    value_loss           | 74          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -20.6        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1463         |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074908715 |\n",
      "|    clip_fraction        | 0.0256       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.000825    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 42.5         |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.00128     |\n",
      "|    value_loss           | 78.1         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -21         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1459        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 15          |\n",
      "|    total_timesteps      | 22528       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010400839 |\n",
      "|    clip_fraction        | 0.0555      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.000259    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.9        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.00555    |\n",
      "|    value_loss           | 91.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -19.8       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1443        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.003735127 |\n",
      "|    clip_fraction        | 0.0529      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -0.00776    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.7        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | -0.00429    |\n",
      "|    value_loss           | 72.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -25.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1440        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 18          |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008676189 |\n",
      "|    clip_fraction        | 0.0406      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 6.77e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 43.7        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.00216    |\n",
      "|    value_loss           | 80.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -24.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1430        |\n",
      "|    iterations           | 14          |\n",
      "|    time_elapsed         | 20          |\n",
      "|    total_timesteps      | 28672       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008454405 |\n",
      "|    clip_fraction        | 0.0599      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.000426    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.4        |\n",
      "|    n_updates            | 130         |\n",
      "|    policy_gradient_loss | -0.00401    |\n",
      "|    value_loss           | 71.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -22.7       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1426        |\n",
      "|    iterations           | 15          |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 30720       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009199837 |\n",
      "|    clip_fraction        | 0.0686      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.00509     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.1        |\n",
      "|    n_updates            | 140         |\n",
      "|    policy_gradient_loss | -0.00346    |\n",
      "|    value_loss           | 76.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -18.6       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1424        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 22          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008321036 |\n",
      "|    clip_fraction        | 0.0315      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00348     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.7        |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.00281    |\n",
      "|    value_loss           | 71.2        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -16.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1424         |\n",
      "|    iterations           | 17           |\n",
      "|    time_elapsed         | 24           |\n",
      "|    total_timesteps      | 34816        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0061770044 |\n",
      "|    clip_fraction        | 0.0608       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.07        |\n",
      "|    explained_variance   | -0.00438     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 28.3         |\n",
      "|    n_updates            | 160          |\n",
      "|    policy_gradient_loss | -0.00424     |\n",
      "|    value_loss           | 68.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -17.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1424        |\n",
      "|    iterations           | 18          |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 36864       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009972331 |\n",
      "|    clip_fraction        | 0.119       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.05       |\n",
      "|    explained_variance   | 0.00681     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 170         |\n",
      "|    policy_gradient_loss | -0.00941    |\n",
      "|    value_loss           | 73.6        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -17.4        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1422         |\n",
      "|    iterations           | 19           |\n",
      "|    time_elapsed         | 27           |\n",
      "|    total_timesteps      | 38912        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0014107648 |\n",
      "|    clip_fraction        | 0.022        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.05        |\n",
      "|    explained_variance   | 0.0147       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 37.5         |\n",
      "|    n_updates            | 180          |\n",
      "|    policy_gradient_loss | -0.00125     |\n",
      "|    value_loss           | 83.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -16.9       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1423        |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 28          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010226514 |\n",
      "|    clip_fraction        | 0.117       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | -0.0376     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 37.3        |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.00595    |\n",
      "|    value_loss           | 69.9        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -14.7        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1421         |\n",
      "|    iterations           | 21           |\n",
      "|    time_elapsed         | 30           |\n",
      "|    total_timesteps      | 43008        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070245974 |\n",
      "|    clip_fraction        | 0.0615       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.00017      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 30.3         |\n",
      "|    n_updates            | 200          |\n",
      "|    policy_gradient_loss | -0.00377     |\n",
      "|    value_loss           | 67           |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -8.22        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1418         |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 31           |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0074760066 |\n",
      "|    clip_fraction        | 0.127        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | -0.00322     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 34.4         |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.0086      |\n",
      "|    value_loss           | 69.5         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -4.42        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1417         |\n",
      "|    iterations           | 23           |\n",
      "|    time_elapsed         | 33           |\n",
      "|    total_timesteps      | 47104        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066564176 |\n",
      "|    clip_fraction        | 0.103        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.03        |\n",
      "|    explained_variance   | 0.00187      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 26           |\n",
      "|    n_updates            | 220          |\n",
      "|    policy_gradient_loss | -0.00919     |\n",
      "|    value_loss           | 53.7         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 0.88         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1415         |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 34           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0073624766 |\n",
      "|    clip_fraction        | 0.096        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.01        |\n",
      "|    explained_variance   | -0.00246     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 19.7         |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00737     |\n",
      "|    value_loss           | 47.8         |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 0.5          |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1410         |\n",
      "|    iterations           | 25           |\n",
      "|    time_elapsed         | 36           |\n",
      "|    total_timesteps      | 51200        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0066135447 |\n",
      "|    clip_fraction        | 0.0949       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.02        |\n",
      "|    explained_variance   | 0.00343      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 36.3         |\n",
      "|    n_updates            | 240          |\n",
      "|    policy_gradient_loss | -0.00346     |\n",
      "|    value_loss           | 61.6         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 3.68        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1408        |\n",
      "|    iterations           | 26          |\n",
      "|    time_elapsed         | 37          |\n",
      "|    total_timesteps      | 53248       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011926004 |\n",
      "|    clip_fraction        | 0.207       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -0.0159     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 250         |\n",
      "|    policy_gradient_loss | -0.0215     |\n",
      "|    value_loss           | 46.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 8.86        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1409        |\n",
      "|    iterations           | 27          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 55296       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007751353 |\n",
      "|    clip_fraction        | 0.118       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.00132     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.2        |\n",
      "|    n_updates            | 260         |\n",
      "|    policy_gradient_loss | -0.0034     |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 17.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1404        |\n",
      "|    iterations           | 28          |\n",
      "|    time_elapsed         | 40          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010445373 |\n",
      "|    clip_fraction        | 0.155       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | 0.00043     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.4        |\n",
      "|    n_updates            | 270         |\n",
      "|    policy_gradient_loss | -0.016      |\n",
      "|    value_loss           | 46.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 25.3        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1402        |\n",
      "|    iterations           | 29          |\n",
      "|    time_elapsed         | 42          |\n",
      "|    total_timesteps      | 59392       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.016398905 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.03       |\n",
      "|    explained_variance   | 0.000291    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 12.6        |\n",
      "|    n_updates            | 280         |\n",
      "|    policy_gradient_loss | 0.00162     |\n",
      "|    value_loss           | 29.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 29.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1400        |\n",
      "|    iterations           | 30          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 61440       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.004613715 |\n",
      "|    clip_fraction        | 0.159       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.02       |\n",
      "|    explained_variance   | -7.87e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21          |\n",
      "|    n_updates            | 290         |\n",
      "|    policy_gradient_loss | 0.00444     |\n",
      "|    value_loss           | 40.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 33          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1396        |\n",
      "|    iterations           | 31          |\n",
      "|    time_elapsed         | 45          |\n",
      "|    total_timesteps      | 63488       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013149339 |\n",
      "|    clip_fraction        | 0.174       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.000177    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.9        |\n",
      "|    n_updates            | 300         |\n",
      "|    policy_gradient_loss | -0.00305    |\n",
      "|    value_loss           | 48.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 34.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1394        |\n",
      "|    iterations           | 32          |\n",
      "|    time_elapsed         | 47          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013879511 |\n",
      "|    clip_fraction        | 0.164       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 7.22e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25          |\n",
      "|    n_updates            | 310         |\n",
      "|    policy_gradient_loss | 0.00062     |\n",
      "|    value_loss           | 45          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 32.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1394        |\n",
      "|    iterations           | 33          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 67584       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009424684 |\n",
      "|    clip_fraction        | 0.137       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | -6.54e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 24.9        |\n",
      "|    n_updates            | 320         |\n",
      "|    policy_gradient_loss | 0.00462     |\n",
      "|    value_loss           | 52.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 32.8        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1391        |\n",
      "|    iterations           | 34          |\n",
      "|    time_elapsed         | 50          |\n",
      "|    total_timesteps      | 69632       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012633093 |\n",
      "|    clip_fraction        | 0.219       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1          |\n",
      "|    explained_variance   | 8.16e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 330         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 54.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | 33.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1389         |\n",
      "|    iterations           | 35           |\n",
      "|    time_elapsed         | 51           |\n",
      "|    total_timesteps      | 71680        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0086322455 |\n",
      "|    clip_fraction        | 0.18         |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.99        |\n",
      "|    explained_variance   | 0.000131     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 24.7         |\n",
      "|    n_updates            | 340          |\n",
      "|    policy_gradient_loss | 0.00318      |\n",
      "|    value_loss           | 51.2         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 36.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1387        |\n",
      "|    iterations           | 36          |\n",
      "|    time_elapsed         | 53          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015877068 |\n",
      "|    clip_fraction        | 0.183       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.987      |\n",
      "|    explained_variance   | -1.3e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 23.9        |\n",
      "|    n_updates            | 350         |\n",
      "|    policy_gradient_loss | 0.00688     |\n",
      "|    value_loss           | 54.1        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 36.8       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1384       |\n",
      "|    iterations           | 37         |\n",
      "|    time_elapsed         | 54         |\n",
      "|    total_timesteps      | 75776      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01781317 |\n",
      "|    clip_fraction        | 0.227      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.979     |\n",
      "|    explained_variance   | 5.18e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 21.5       |\n",
      "|    n_updates            | 360        |\n",
      "|    policy_gradient_loss | -0.00653   |\n",
      "|    value_loss           | 57.3       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 37          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1381        |\n",
      "|    iterations           | 38          |\n",
      "|    time_elapsed         | 56          |\n",
      "|    total_timesteps      | 77824       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012323738 |\n",
      "|    clip_fraction        | 0.196       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.985      |\n",
      "|    explained_variance   | 1.5e-05     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 27.8        |\n",
      "|    n_updates            | 370         |\n",
      "|    policy_gradient_loss | 0.00052     |\n",
      "|    value_loss           | 53          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 42.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1378        |\n",
      "|    iterations           | 39          |\n",
      "|    time_elapsed         | 57          |\n",
      "|    total_timesteps      | 79872       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.017261598 |\n",
      "|    clip_fraction        | 0.225       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.972      |\n",
      "|    explained_variance   | 2.51e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 20.9        |\n",
      "|    n_updates            | 380         |\n",
      "|    policy_gradient_loss | 0.00442     |\n",
      "|    value_loss           | 58.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1379        |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 59          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008432878 |\n",
      "|    clip_fraction        | 0.166       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.983      |\n",
      "|    explained_variance   | 3.93e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.3        |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | 0.0071      |\n",
      "|    value_loss           | 70.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 48.2        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1378        |\n",
      "|    iterations           | 41          |\n",
      "|    time_elapsed         | 60          |\n",
      "|    total_timesteps      | 83968       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.018408109 |\n",
      "|    clip_fraction        | 0.162       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.977      |\n",
      "|    explained_variance   | 1.49e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 25.5        |\n",
      "|    n_updates            | 400         |\n",
      "|    policy_gradient_loss | 0.00644     |\n",
      "|    value_loss           | 77.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 45.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1377        |\n",
      "|    iterations           | 42          |\n",
      "|    time_elapsed         | 62          |\n",
      "|    total_timesteps      | 86016       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023577502 |\n",
      "|    clip_fraction        | 0.26        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | 4.71e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 50.3        |\n",
      "|    n_updates            | 410         |\n",
      "|    policy_gradient_loss | 0.016       |\n",
      "|    value_loss           | 83.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 46.4        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1377        |\n",
      "|    iterations           | 43          |\n",
      "|    time_elapsed         | 63          |\n",
      "|    total_timesteps      | 88064       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015912166 |\n",
      "|    clip_fraction        | 0.256       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | -4.05e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 420         |\n",
      "|    policy_gradient_loss | 0.0101      |\n",
      "|    value_loss           | 79          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 46.1        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1377        |\n",
      "|    iterations           | 44          |\n",
      "|    time_elapsed         | 65          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013290007 |\n",
      "|    clip_fraction        | 0.262       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.958      |\n",
      "|    explained_variance   | -2.86e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 36.9        |\n",
      "|    n_updates            | 430         |\n",
      "|    policy_gradient_loss | 0.0168      |\n",
      "|    value_loss           | 78.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 48          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1376        |\n",
      "|    iterations           | 45          |\n",
      "|    time_elapsed         | 66          |\n",
      "|    total_timesteps      | 92160       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.030551586 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.961      |\n",
      "|    explained_variance   | -2.74e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 48.3        |\n",
      "|    n_updates            | 440         |\n",
      "|    policy_gradient_loss | 0.00992     |\n",
      "|    value_loss           | 81.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 43          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1376        |\n",
      "|    iterations           | 46          |\n",
      "|    time_elapsed         | 68          |\n",
      "|    total_timesteps      | 94208       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021658834 |\n",
      "|    clip_fraction        | 0.236       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.965      |\n",
      "|    explained_variance   | -2.5e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 450         |\n",
      "|    policy_gradient_loss | 0.00908     |\n",
      "|    value_loss           | 83.9        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 60         |\n",
      "|    ep_rew_mean          | 42.9       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1377       |\n",
      "|    iterations           | 47         |\n",
      "|    time_elapsed         | 69         |\n",
      "|    total_timesteps      | 96256      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.01083851 |\n",
      "|    clip_fraction        | 0.202      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.956     |\n",
      "|    explained_variance   | -6.68e-06  |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 39.6       |\n",
      "|    n_updates            | 460        |\n",
      "|    policy_gradient_loss | -0.00314   |\n",
      "|    value_loss           | 74.7       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 44.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1377        |\n",
      "|    iterations           | 48          |\n",
      "|    time_elapsed         | 71          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021943744 |\n",
      "|    clip_fraction        | 0.272       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | -8.94e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 35.5        |\n",
      "|    n_updates            | 470         |\n",
      "|    policy_gradient_loss | 0.0128      |\n",
      "|    value_loss           | 73.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 49.6        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1376        |\n",
      "|    iterations           | 49          |\n",
      "|    time_elapsed         | 72          |\n",
      "|    total_timesteps      | 100352      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.021008562 |\n",
      "|    clip_fraction        | 0.182       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.952      |\n",
      "|    explained_variance   | -1.43e-06   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 34.7        |\n",
      "|    n_updates            | 480         |\n",
      "|    policy_gradient_loss | 0.0098      |\n",
      "|    value_loss           | 83.2        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1fdb099b790>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "env=ShowerEnv() # redefining the environment\n",
    "# env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVecEnv for vectorized training\n",
    "# Create the PPO model with the specified policy and environment\n",
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=100000)\n",
    "# model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ccb1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Shower Model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_ShowerEnv_TS_1000000\")\n",
    "model.save(PPO_Path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "67c94981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59.48, 0.8772684879784522)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model  # Delete the model to demonstrate loading\n",
    "model = PPO.load(PPO_Path, env=env) # Load the model using the same path as above\n",
    "evaluate_policy(model, env, n_eval_episodes=50, render=True) # Evaluate policy takes arguments as , model, env. no. of episodes to evaluate, and render mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfed4b7d",
   "metadata": {},
   "source": [
    "### Testing Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74dad3fb",
   "metadata": {},
   "source": [
    "- It is point to be noted that\n",
    "- Model is trained really well can see from the log chart as well as evaluate policy\n",
    "- when you increase the random value generation limits, you can notice your rewards goes down as it cannot achieve to the max reward within 60 time step\n",
    "- PLay with the rand intitalization value in the self.reset and see different values of the rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9746e9bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 finished with score: 34\n",
      "Episode 2 finished with score: 12\n",
      "Episode 3 finished with score: 48\n",
      "Episode 4 finished with score: 38\n",
      "Episode 5 finished with score: 34\n",
      "Episode 6 finished with score: 6\n",
      "Episode 7 finished with score: 2\n",
      "Episode 8 finished with score: 18\n",
      "Episode 9 finished with score: 12\n",
      "Episode 10 finished with score: 26\n",
      "Episode 11 finished with score: 8\n",
      "Episode 12 finished with score: 32\n",
      "Episode 13 finished with score: 28\n",
      "Episode 14 finished with score: 46\n",
      "Episode 15 finished with score: 14\n",
      "Episode 16 finished with score: 26\n",
      "Episode 17 finished with score: 38\n",
      "Episode 18 finished with score: 54\n",
      "Episode 19 finished with score: -2\n",
      "Episode 20 finished with score: 4\n",
      "Episode 21 finished with score: 42\n",
      "Episode 22 finished with score: 6\n",
      "Episode 23 finished with score: 36\n",
      "Episode 24 finished with score: 36\n",
      "Episode 25 finished with score: 24\n",
      "Episode 26 finished with score: 36\n",
      "Episode 27 finished with score: 20\n",
      "Episode 28 finished with score: 48\n",
      "Episode 29 finished with score: 2\n",
      "Episode 30 finished with score: 20\n",
      "Episode 31 finished with score: 16\n",
      "Episode 32 finished with score: 36\n",
      "Episode 33 finished with score: 40\n",
      "Episode 34 finished with score: -6\n",
      "Episode 35 finished with score: 4\n",
      "Episode 36 finished with score: 42\n",
      "Episode 37 finished with score: 4\n",
      "Episode 38 finished with score: 22\n",
      "Episode 39 finished with score: 40\n",
      "Episode 40 finished with score: 36\n",
      "Episode 41 finished with score: 40\n",
      "Episode 42 finished with score: 18\n",
      "Episode 43 finished with score: 6\n",
      "Episode 44 finished with score: 14\n",
      "Episode 45 finished with score: 34\n",
      "Episode 46 finished with score: 22\n",
      "Episode 47 finished with score: 42\n",
      "Episode 48 finished with score: 40\n",
      "Episode 49 finished with score: 10\n",
      "Episode 50 finished with score: 0\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "env=ShowerEnv()\n",
    "episode=50\n",
    "for episode in range(1, episode + 1): #for loop to run the simulation for number of episodes\n",
    "    # Reset the environment for each episode\n",
    "    obs, info = env.reset()\n",
    "    # Initialize done variable to False which is used to raise flage of termination\n",
    "    done = False\n",
    "    # Variable to keep track of the score; Score is cumulative reward\n",
    "    score=0\n",
    "    # Loop until the episode is done (or not terminated)\n",
    "    #we could have also used max reward or max time steps to terminate the episode\n",
    "    while not done:\n",
    "        # Sample a random action from the action space\n",
    "        # In a real scenario, you would use a trained model to predict the action\n",
    "        action, _= model.predict(obs)  # Random action for demonstration\n",
    "        # Take a step in the environment with the sampled action\n",
    "        # The step function returns the next observation, reward, done flag, info dictionary and the time step\n",
    "        obs, reward, done, truncated, info = env.step(action)\n",
    "        # render the environment to visualize the action taken\n",
    "        env.render()\n",
    "        #keep the score of the episode\n",
    "        score += reward\n",
    "        # print(f\"Episode: {episode}, Score: {score}, Action: {action}, Reward: {reward}\")\n",
    "    print(f\"Episode {episode} finished with score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065245f3",
   "metadata": {},
   "source": [
    "### Vectorising the custom Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac0227",
   "metadata": {},
   "source": [
    "- Setting up the log path\n",
    "- vectorzoing it \n",
    "- defining a model\n",
    "- training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ebafda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "log_path = os.path.join(\"Training\", \"Logs\")\n",
    "env = make_vec_env(lambda: ShowerEnv(), n_envs=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efe9f910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to Training\\Logs\\PPO_9\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 60       |\n",
      "|    ep_rew_mean     | -41.6    |\n",
      "| time/              |          |\n",
      "|    fps             | 8512     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -43.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 3282         |\n",
      "|    iterations           | 2            |\n",
      "|    time_elapsed         | 4            |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0020263607 |\n",
      "|    clip_fraction        | 0.0298       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | -0.000313    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 22.2         |\n",
      "|    n_updates            | 10           |\n",
      "|    policy_gradient_loss | -0.00183     |\n",
      "|    value_loss           | 49.3         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -40.3       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2715        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 9           |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005256478 |\n",
      "|    clip_fraction        | 0.0242      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.09       |\n",
      "|    explained_variance   | 0.0036      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 28.6        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00102    |\n",
      "|    value_loss           | 58.8        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 60           |\n",
      "|    ep_rew_mean          | -30.8        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2473         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 13           |\n",
      "|    total_timesteps      | 32768        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0049237353 |\n",
      "|    clip_fraction        | 0.0585       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -1.09        |\n",
      "|    explained_variance   | 0.00444      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 45.4         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.00167     |\n",
      "|    value_loss           | 77.4         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -26.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2346        |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 17          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008054329 |\n",
      "|    clip_fraction        | 0.0803      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | -3.25e-05   |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 39.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0057     |\n",
      "|    value_loss           | 81.5        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -16.5       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2268        |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 21          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007873966 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.07       |\n",
      "|    explained_variance   | 0.00267     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 40.2        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0064     |\n",
      "|    value_loss           | 73.3        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -14.1       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2213        |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 25          |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006414354 |\n",
      "|    clip_fraction        | 0.0688      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.06       |\n",
      "|    explained_variance   | 0.0347      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 32.3        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00593    |\n",
      "|    value_loss           | 68.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | -1.14       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2175        |\n",
      "|    iterations           | 8           |\n",
      "|    time_elapsed         | 30          |\n",
      "|    total_timesteps      | 65536       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009792146 |\n",
      "|    clip_fraction        | 0.106       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.04       |\n",
      "|    explained_variance   | 0.0235      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 30.4        |\n",
      "|    n_updates            | 70          |\n",
      "|    policy_gradient_loss | -0.00659    |\n",
      "|    value_loss           | 59.6        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 0.8         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2116        |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 34          |\n",
      "|    total_timesteps      | 73728       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011151432 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.01       |\n",
      "|    explained_variance   | 0.0442      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.2        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00982    |\n",
      "|    value_loss           | 38.9        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 14.7        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2087        |\n",
      "|    iterations           | 10          |\n",
      "|    time_elapsed         | 39          |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.015196206 |\n",
      "|    clip_fraction        | 0.198       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.996      |\n",
      "|    explained_variance   | 0.105       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 18.9        |\n",
      "|    n_updates            | 90          |\n",
      "|    policy_gradient_loss | -0.0146     |\n",
      "|    value_loss           | 32.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 25.5        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2064        |\n",
      "|    iterations           | 11          |\n",
      "|    time_elapsed         | 43          |\n",
      "|    total_timesteps      | 90112       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.012409221 |\n",
      "|    clip_fraction        | 0.173       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.967      |\n",
      "|    explained_variance   | 0.131       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.5        |\n",
      "|    n_updates            | 100         |\n",
      "|    policy_gradient_loss | -0.0107     |\n",
      "|    value_loss           | 25.1        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 30          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2042        |\n",
      "|    iterations           | 12          |\n",
      "|    time_elapsed         | 48          |\n",
      "|    total_timesteps      | 98304       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.023038335 |\n",
      "|    clip_fraction        | 0.235       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.94       |\n",
      "|    explained_variance   | 0.0657      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 11.1        |\n",
      "|    n_updates            | 110         |\n",
      "|    policy_gradient_loss | 0.00121     |\n",
      "|    value_loss           | 24.2        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 60          |\n",
      "|    ep_rew_mean          | 28.9        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 2023        |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 52          |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.022270786 |\n",
      "|    clip_fraction        | 0.23        |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.921      |\n",
      "|    explained_variance   | 0.0197      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.1        |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | 0.00199     |\n",
      "|    value_loss           | 29.4        |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x1ef6bb037d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model= PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)\n",
    "model.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a431ac81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the Shower Model\n",
    "PPO_Path = os.path.join(\"Training\", \"Saved Models\", \"PPO_Vec_env_ShowerEnv_TS_1000000\")\n",
    "model.save(PPO_Path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3a121fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(53.14, 5.379628239943723)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del model  # Delete the model to demonstrate loading\n",
    "model = PPO.load(PPO_Path, env=ShowerEnv()) # Load the model using the same path as above\n",
    "evaluate_policy(model, env, n_eval_episodes=200, render=True) # Evaluate policy takes arguments as , model, env. no. of episodes to evaluate, and render mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a190de2b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
